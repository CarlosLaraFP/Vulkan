#define GLFW_INCLUDE_VULKAN
#include <GLFW/glfw3.h> // GLFW will include its own definitions and automatically load <vulkan/vulkan.h>

#define STB_IMAGE_IMPLEMENTATION
#include <stb_image.h>

#define TINYOBJLOADER_IMPLEMENTATION
#include <tiny_obj_loader.h>

// necessary to make sure that functions like glm::rotate use radians as arguments
#define GLM_FORCE_RADIANS
// The perspective projection matrix generated by GLM will use the OpenGL depth range of -1.0 to 1.0 by default. 
// We need to configure it to use the Vulkan range of 0.0 to 1.0
#define GLM_FORCE_DEPTH_ZERO_TO_ONE
#include <glm/glm.hpp> // linear algebra related types like vectors and matrices
/*
    Exposes functions that can be used to generate model transformations like glm::rotate, 
    view transformations like glm::lookAt, and projection transformations like glm::perspective.
*/
#include <glm/gtc/matrix_transform.hpp>
// Exposes functions to do precise timekeeping (i.e. make sure that the geometry rotates 90 degrees per second regardless of frame rate).
#include <chrono>

#include <iostream>
#include <stdexcept>
#include <cstdlib> // provides the EXIT_SUCCESS and EXIT_FAILURE macros
#include <vector>
#include <algorithm> //std::clamp and std::find_if; C++20 has the more concise std::ranges::find in <ranges>
#include <sstream> // std::ostringstream; C++20 has std::format in <format>
#include <cstring> // strcmp compares two strings character by character. If the strings are equal, the function returns 0.
#include <optional> // C++17
#include <set>
#include <cstdint> // uint32_t
#include <limits> // std::numeric_limits
#include <fstream>
#include <array>
#include <unordered_map>

// hash functions for the GLM types need to be included
#define GLM_ENABLE_EXPERIMENTAL
#include <glm/gtx/hash.hpp>

const uint32_t WIDTH = 800;
const uint32_t HEIGHT = 600;
/*
    We choose the number 2 because we don’t want the CPU to get too far ahead of the GPU. With 2 frames in flight, the CPU and the GPU can 
    be working on their own tasks at the same time. If the CPU finishes early, it will wait till the GPU finishes rendering before submitting 
    more work. With 3 or more frames in flight, the CPU could get ahead of the GPU, adding frames of latency. Generally, extra latency isn’t 
    desired. But giving the application control over the number of frames in flight is another example of Vulkan being explicit.
*/
const int MAX_FRAMES_IN_FLIGHT = 2;

const std::string MODEL_PATH = "models/viking_room.obj";
const std::string TEXTURE_PATH = "textures/viking_room.png";

// All of the useful standard validation is bundled into a layer included in the SDK:
const std::vector<const char*> validationLayers = { "VK_LAYER_KHRONOS_validation" };

// Required device extensions in addition to instance extensions (GLFW and debug)
/*
    Not all graphics cards are capable of presenting images directly to a screen for various reasons, for example because 
    they are designed for servers and don’t have any display outputs. Secondly, since image presentation is heavily tied 
    into the window system and the surfaces associated with windows, it is not actually part of the Vulkan core. 
    You have to enable the VK_KHR_swapchain device extension after querying for its support.
*/
const std::vector<const char*> deviceExtensions = { VK_KHR_SWAPCHAIN_EXTENSION_NAME }; // macro helps the compiler catch misspellings

// The NDEBUG macro is part of the C++ standard and means "not debug"
#ifdef NDEBUG
    const bool enableValidationLayers = false;
#else
    const bool enableValidationLayers = true;
#endif

/*
    VK_DEBUG_UTILS_MESSAGE_SEVERITY_VERBOSE_BIT_EXT: Diagnostic message
    VK_DEBUG_UTILS_MESSAGE_SEVERITY_INFO_BIT_EXT: Informational message like the creation of a resource
    VK_DEBUG_UTILS_MESSAGE_SEVERITY_WARNING_BIT_EXT: Message about behavior that is not necessarily an error, but very likely a bug in your application
    VK_DEBUG_UTILS_MESSAGE_SEVERITY_ERROR_BIT_EXT: Message about behavior that is invalid and may cause crashes

    The values of this enumeration are set up in such a way that you can use a comparison operation to check 
    if a message is equal or worse compared to some level of severity, for example:

    if (messageSeverity >= VK_DEBUG_UTILS_MESSAGE_SEVERITY_WARNING_BIT_EXT) 
    {
        // Message is important enough to show
    }

    The messageType parameter can have the following values:

    VK_DEBUG_UTILS_MESSAGE_TYPE_GENERAL_BIT_EXT: Some event has happened that is unrelated to the specification or performance
    VK_DEBUG_UTILS_MESSAGE_TYPE_VALIDATION_BIT_EXT: Something has happened that violates the specification or indicates a possible mistake
    VK_DEBUG_UTILS_MESSAGE_TYPE_PERFORMANCE_BIT_EXT: Potential non-optimal use of Vulkan
*/
static VKAPI_ATTR VkBool32 VKAPI_CALL debugCallback(
    VkDebugUtilsMessageSeverityFlagBitsEXT messageSeverity,
    VkDebugUtilsMessageTypeFlagsEXT messageType,
    const VkDebugUtilsMessengerCallbackDataEXT* pCallbackData,
    void* pUserData // specified during the setup of the callback and allows you to pass your own data to it
) {
    std::cerr << "Validation Layer: " << pCallbackData->pMessage << std::endl;

    /*
        The callback returns a boolean that indicates if the Vulkan call that triggered the validation layer message should be aborted. 
        If the callback returns true, then the call is aborted with the VK_ERROR_VALIDATION_FAILED_EXT error. 
        This is normally only used to test the validation layers themselves, so you should always return VK_FALSE.
    */
    return VK_FALSE;
}

/*
    Extensions in Vulkan, such as VkDebugUtilsMessengerEXT, play a critical role in extending the core functionality of Vulkan. Vulkan is designed to be a lean and efficient graphics and compute API, offering minimal features in its core specification to cover a wide range of hardware. Extensions allow developers to access additional features and tools that are not part of the core specification, enabling them to use new hardware capabilities, debugging tools, and other functionalities that may be vendor-specific or not universally needed.

    Understanding 

    The VkDebugUtilsMessengerEXT extension is a part of the Vulkan API that provides a way to receive callbacks from the Vulkan driver for various events, such as errors, warnings, and performance issues. It is extremely useful during development for debugging and validating applications' Vulkan usage.

    Why Extensions Need to be Loaded Dynamically

    Extensions like VkDebugUtilsMessengerEXT need to be loaded dynamically for several reasons:

    Version and Vendor Independence: Vulkan aims to be a cross-platform API, supporting a wide range of hardware and software environments. Extensions can be vendor-specific or only available in certain versions of a driver or operating system. Dynamically loading extensions allows applications to query and use these features only when they are available, making the application more portable and robust.

    Avoiding Bloat: Including all possible extensions and their functionalities directly in the Vulkan library would significantly increase its size and complexity. By requiring explicit loading of extensions, Vulkan keeps the core library lean and efficient. Developers only use and load what is necessary for their application.

    Forward Compatibility: Dynamically loading extensions ensures that applications can run on a wide variety of hardware and software configurations, including future ones. An application can check for the presence of an extension and adapt its behavior accordingly, whether the extension is available or not.

    How Extensions are Loaded

    The methods below for loading/destroying the VkDebugUtilsMessengerEXT extension is a common pattern for working with Vulkan extensions:

    vkGetInstanceProcAddr: This function retrieves the address of the extension function from the Vulkan loader or driver. By passing the instance and the name of the function, it returns a function pointer (PFN_vkCreateDebugUtilsMessengerEXT in this case) that can be cast to the appropriate type.

    Function Pointer Call: If the function pointer is not nullptr, the extension is present, and the application can call the function to use the extension's features. Otherwise, it indicates that the extension is not available (VK_ERROR_EXTENSION_NOT_PRESENT), and the application can handle this situation, usually by disabling the functionality associated with the extension or falling back to a different implementation.

    This dynamic loading mechanism allows Vulkan applications to be both forward-compatible and adaptable to the wide range of hardware and software environments where they might run.

    Vulkan extensions can be thought of as additional features or functionalities that are not part of the Vulkan core specification. They can be provided by various sources:

    GPU Vendors (Hardware): Many Vulkan extensions are specific to hardware from certain GPU vendors, such as NVIDIA, AMD, or Intel. These extensions allow developers to take advantage of unique features and capabilities of these vendors' GPUs. For example, an extension might expose a new rendering technique or optimization that is only possible on a particular vendor's hardware.

    Khronos Group (Vulkan's Governing Body): Some extensions are standardized by the Khronos Group, the consortium behind Vulkan. These extensions may add features that are useful across a wide range of hardware but were not included in the core Vulkan specification for various reasons, possibly including the need for further experimentation or because they were developed after the core specification was finalized.

    Platform-Specific: There are also extensions that are specific to certain operating systems or platforms. These extensions allow Vulkan applications to better integrate with the underlying system, offering functionalities like better window system integration or specific optimizations for a given platform.

    When an extension is available on your machine, it means that the combination of your GPU hardware, its driver, and possibly the operating system supports this extension. The Vulkan driver for your GPU implements these extensions to expose additional features beyond what is available in the Vulkan core. This is why not all extensions are available on all devices; their availability can depend on the hardware capability of the GPU, the driver version installed, and sometimes the operating system version.

    The process of dynamically querying and loading extensions, as described earlier, allows a Vulkan application to check at runtime which extensions are available on the user's machine and adapt its behavior accordingly. This ensures that the application can take advantage of these extended features when available, while still being able to run on systems where those features are not supported.
*/

/*
    Because this function is an extension function, it is not automatically loaded. 
    We have to look up its address ourselves using vkGetInstanceProcAddr via a proxy function that handles this in the background.
*/
static VkResult CreateDebugUtilsMessengerEXT(
    VkInstance instance, 
    const VkDebugUtilsMessengerCreateInfoEXT* pCreateInfo, 
    const VkAllocationCallbacks* pAllocator, 
    VkDebugUtilsMessengerEXT* pDebugMessenger
) {
    auto func = (PFN_vkCreateDebugUtilsMessengerEXT)vkGetInstanceProcAddr(instance, "vkCreateDebugUtilsMessengerEXT");

    if (func != nullptr) 
    {
        return func(instance, pCreateInfo, pAllocator, pDebugMessenger);
    }
    else 
    {
        return VK_ERROR_EXTENSION_NOT_PRESENT;
    }
}

/*
    The VkDebugUtilsMessengerEXT object also needs to be cleaned up with a call to vkDestroyDebugUtilsMessengerEXT. 
    Similarly to vkCreateDebugUtilsMessengerEXT the function needs to be explicitly loaded
*/
static void DestroyDebugUtilsMessengerEXT(
    VkInstance instance, 
    VkDebugUtilsMessengerEXT debugMessenger, 
    const VkAllocationCallbacks* pAllocator
) {
    auto func = (PFN_vkDestroyDebugUtilsMessengerEXT)vkGetInstanceProcAddr(instance, "vkDestroyDebugUtilsMessengerEXT");

    if (func != nullptr) 
    {
        func(instance, debugMessenger, pAllocator);
    }
}

// Reads all of the bytes from the specified file and returns them in a byte array managed by a vector.
static std::vector<char> readFile(const std::string& filename)
{
    // ate: Start reading at the end of the file
    // binary: Read the file as a binary file (avoid text transformations)
    std::ifstream file { filename, std::ios::ate | std::ios::binary };

    if (!file.is_open())
    {
        throw std::runtime_error("Failed to open file.");
    }

    // The advantage of starting to read at the end of the file is that we can use 
    // the read position to determine the size of the file and allocate a buffer:
    size_t fileSize = (size_t)file.tellg(); // returns the current position of the file pointer (end, which = file size)
    // the distinction between an initializer list and a single size argument matters for std::vector (be careful)
    std::vector<char> buffer(fileSize);

    file.seekg(0); // Resets the file pointer to the beginning of the file, preparing it for reading from the start.
    file.read(buffer.data(), fileSize); // reads fileSize bytes from the file into the buffer
    file.close();

    return buffer;
}

struct QueueFamilyIndices
{
    /*
        It’s possible that the queue families supporting drawing commands and the ones supporting presentation do not overlap. 
        Therefore we have to take into account that there could be a distinct presentation queue.
    */
    std::optional<uint32_t> graphicsFamily;
    std::optional<uint32_t> presentFamily;

    bool isComplete() const
    {
        return graphicsFamily.has_value() && presentFamily.has_value();
    }
};

/*
    There are basically three kinds of properties we need to check:

    - Basic surface capabilities (min/max number of images in swap chain, min/max width and height of images)
    - Surface formats (pixel format, color space)
    - Available presentation modes
*/
struct SwapChainSupportDetails
{
    /*
        The VkSurfaceCapabilitiesKHR structure includes minImageExtent and maxImageExtent fields that define 
        the minimum and maximum dimensions of swap chain images that the device supports for the surface in question.
    */
    VkSurfaceCapabilitiesKHR capabilities;
    std::vector<VkSurfaceFormatKHR> formats;
    std::vector<VkPresentModeKHR> presentModes;
};

// Interleaving vertex attributes
struct Vertex
{
    glm::vec3 position;
    glm::vec3 color;
    glm::vec2 texture;

    /*
        Tells Vulkan how to pass this data format to the vertex shader once it’s been uploaded into GPU memory.
        A vertex binding describes at which rate to load data from memory throughout the vertices. It specifies the number 
        of bytes between data entries and whether to move to the next data entry after each vertex or after each instance.
    */
    static VkVertexInputBindingDescription getBindingDescription()
    {
        VkVertexInputBindingDescription bindingDescription {};
        // All of our per-vertex data is packed together in one array, so we’re only going to have one binding.
        // The binding parameter specifies the index of the binding in the array of bindings
        bindingDescription.binding = 0;
        // specifies the number of bytes from one entry to the next
        bindingDescription.stride = sizeof(Vertex);
        /*
            Specifies whether vertex attribute addressing is a function of the vertex index or of the instance index.

            VK_VERTEX_INPUT_RATE_VERTEX: Move to the next data entry after each vertex
            VK_VERTEX_INPUT_RATE_INSTANCE: Move to the next data entry after each instance

            We’re not going to use instanced rendering, so we’ll stick to per-vertex data.
        */
        bindingDescription.inputRate = VK_VERTEX_INPUT_RATE_VERTEX;

        return bindingDescription;
    }

    /*
        Describes how to handle vertex input.
        Describes how to extract a vertex attribute from a chunk of vertex data originating from a binding description.
        The format parameter describes the type of data for the attribute. They are specified using the same enumeration as color formats.
        The format parameter implicitly defines the byte size of attribute data and 
        the offset parameter specifies the number of bytes since the start of the per-vertex data to read from.
    */
    static std::array<VkVertexInputAttributeDescription, 3> getAttributeDescriptions()
    {
        // This array type is basically a vector with a known size at compile time.
        std::array<VkVertexInputAttributeDescription, 3> attributeDescriptions {};

        // layout(location = 0) in vertex shader
        attributeDescriptions[0].location = 0;
        // the binding number which this attribute takes its data from
        attributeDescriptions[0].binding = 0;
        // the size and type of the vertex attribute data
        attributeDescriptions[0].format = VK_FORMAT_R32G32B32_SFLOAT;
        // byte offset of this attribute relative to the start of an element in the vertex input binding
        attributeDescriptions[0].offset = offsetof(Vertex, position);

        // layout(location = 1) in vertex shader
        attributeDescriptions[1].location = 1;
        attributeDescriptions[1].binding = 0;
        attributeDescriptions[1].format = VK_FORMAT_R32G32B32_SFLOAT; // only integers can be unsigned
        attributeDescriptions[1].offset = offsetof(Vertex, color);

        // layout(location = 2) in vertex shader
        attributeDescriptions[2].location = 2;
        attributeDescriptions[2].binding = 0;
        attributeDescriptions[2].format = VK_FORMAT_R32G32_SFLOAT;
        attributeDescriptions[2].offset = offsetof(Vertex, texture);

        return attributeDescriptions;
    }

    bool operator==(const Vertex& other) const 
    {
        return position == other.position && color == other.color && texture == other.texture;
    }
};

/*
    A hash function for Vertex is implemented by specifying a template specialization for std::hash<T>.
    Hash functions are a complex topic, but cppreference.com recommends the following approach combining
    the fields of a struct to create a decent quality hash function:
*/
namespace std
{
    template<> struct hash<Vertex>
    {
        size_t operator()(Vertex const& vertex) const
        {
            return ((hash<glm::vec3>()(vertex.position) ^
                (hash<glm::vec3>()(vertex.color) << 1)) >> 1) ^
                (hash<glm::vec2>()(vertex.texture) << 1);
        }
    };
}

/*
    A descriptor is a way for shaders to freely access resources like buffers and images.

    1. Specify a descriptor set layout during pipeline creation
    2. Allocate a descriptor set from a descriptor pool
    3. Bind the descriptor set during rendering

    We can exactly match the definition in the shader using data types in GLM. The data in the matrices is binary 
    compatible with the way the shader expects it, so we can just memcpy a UniformBufferObject to a VkBuffer.
*/
struct UniformBufferObject 
{
    alignas(16) glm::mat4 model;
    alignas(16) glm::mat4 view;
    alignas(16) glm::mat4 projection;
};
/*
    Vulkan expects the data in our struct to be aligned in memory in a specific way, for example:

    - Scalars have to be aligned by N (= 4 bytes given 32 bit floats).
    - A vec2 must be aligned by 2N (= 8 bytes)
    - A vec3 or vec4 must be aligned by 4N (= 16 bytes)
    - A nested structure must be aligned by the base alignment of its members rounded up to a multiple of 16.
    - A mat4 matrix must have the same alignment as a vec4.

    You can find the full list of alignment requirements in the specification. 
    The Vulkan specification and hardware requirements dictate specific alignment for types when interfacing with shaders.
    The alignas specifier in C++ allows you to specify the alignment requirement of a variable or type. Alignment is a constraint on the 
    address of an object in memory, dictating that the object's address must be a multiple of some byte value, which is the alignment. 
    For instance, an alignment of 16 means that the starting memory address of the variable must be a multiple of 16 bytes.

    Our shader with three mat4 fields already met the alignment requirements. As each mat4 is 4 x 4 x 4 = 64 bytes in size, 
    model has an offset of 0, view has an offset of 64, and projection has an offset of 128. All of these are multiples of 16.
    It's best practice to always be explicit about alignment.
*/

// A lot of information in Vulkan is passed through structs instead of function parameters.
class HelloTriangleApplication 
{
public:
    void run() 
    {
        initWindow();
        initVulkan();
        mainLoop();
        cleanup();
    }

private:
    GLFWwindow* window = nullptr;
    VkInstance instance;
    VkDebugUtilsMessengerEXT debugMessenger; // The debug callback is managed with a handle that needs to be explicitly created and destroyed.
    VkSurfaceKHR surface;
    VkPhysicalDevice physicalDevice = VK_NULL_HANDLE; // (query GPU features) Implicitly destroyed when the VkInstance is destroyed
    VkDevice device; // logical device (use GPU features)
    /*
        The queues are automatically created along with the logical device, but we need a handle to interface with them.
        Device queues are implicitly cleaned up when the logical device is destroyed.
    */
    VkQueue graphicsQueue;
    VkQueue presentQueue;
    VkSwapchainKHR swapChain;
    // Each created by the implementation for the swap chain and will be automatically cleaned up once the swap chain has been destroyed.
    std::vector<VkImage> swapChainImages;
    std::vector<VkImageView> swapChainImageViews; // describes how to access the image and which part of the image to access
    VkFormat swapChainImageFormat; // required for VkImageViewCreateInfo and VkAttachmentDescription
    VkExtent2D swapChainExtent; // required for VkViewport in the graphics pipeline
    VkRenderPass renderPass;
    std::vector<Vertex> vertices;
    VkBuffer vertexBuffer;
    VkDeviceMemory vertexBufferMemory;
    std::vector<uint32_t> indices; // we will use more than 65,535 unique vertices
    VkBuffer indexBuffer;
    VkDeviceMemory indexBufferMemory;
    uint32_t mipLevels;
    VkImage textureImage;
    VkDeviceMemory textureImageMemory;
    VkImageView textureImageView;
    VkSampler textureSampler; // texture filtering state
    VkImage depthImage;
    VkDeviceMemory depthImageMemory;
    VkImageView depthImageView;
    std::vector<VkBuffer> uniformBuffers;
    std::vector<VkDeviceMemory> uniformBuffersMemory;
    std::vector<void*> uniformBuffersMapped;
    VkDescriptorPool descriptorPool;
    std::vector<VkDescriptorSet> descriptorSets;
    VkDescriptorSetLayout descriptorSetLayout;
    VkPipelineLayout pipelineLayout;
    VkPipeline graphicsPipeline;
    std::vector<VkFramebuffer> swapChainFramebuffers;
    VkCommandPool commandPool;
    std::vector<VkCommandBuffer> commandBuffers; // automatically freed when their command pool is destroyed
    std::vector<VkSemaphore> imageAvailableSemaphores;
    std::vector<VkSemaphore> renderFinishedSemaphores;
    std::vector<VkFence> inFlightFences;
    uint32_t currentFrame = 0; // To use the right objects every frame, we need to keep track of the current frame.
    bool framebufferResized = false; // window resized
    /*
    // Position and color values are combined into one array of vertices. This is known as interleaving vertex attributes.
    const std::vector<Vertex> vertices =
    {
        {{-0.5f, -0.5f, 0.0f}, {1.0f, 0.0f, 0.0f}, {0.0f, 1.0f}},
        {{0.5f, -0.5f, 0.0f}, {0.0f, 1.0f, 0.0f}, {1.0f, 1.0f}},
        {{0.5f, 0.5f, 0.0f}, {0.0f, 0.0f, 1.0f}, {1.0f, 0.0f}},
        {{-0.5f, 0.5f, 0.0f}, {1.0f, 1.0f, 1.0f}, {0.0f, 0.0f}},

        {{-0.5f, -0.5f, -0.5f}, {1.0f, 0.0f, 0.0f}, {0.0f, 1.0f}},
        {{0.5f, -0.5f, -0.5f}, {0.0f, 1.0f, 0.0f}, {1.0f, 1.0f}},
        {{0.5f, 0.5f, -0.5f}, {0.0f, 0.0f, 1.0f}, {1.0f, 0.0f}},
        {{-0.5f, 0.5f, -0.5f}, {1.0f, 1.0f, 1.0f}, {0.0f, 0.0f}}
    };
    // uint16_t because we are using less than 65,535 unique vertices
    const std::vector<uint16_t> indices = 
    { 
        0, 1, 2, 2, 3, 0,
        4, 5, 6, 6, 7, 4
    };
    */

    /*
        The reason that we’re creating a static function as a callback is because GLFW doesn't know how to 
        properly call a member function with the right this pointer to our HelloTriangleApplication instance.
        However, we do get a reference to the GLFWwindow in the callback and there is another GLFW function 
        that allows you to store an arbitrary pointer inside of it: glfwSetWindowUserPointer
    */
    static void framebufferResizeCallback(GLFWwindow* window, int width, int height)
    {
        auto app = reinterpret_cast<HelloTriangleApplication*>(glfwGetWindowUserPointer(window));

        app->framebufferResized = true;
    }

    void initWindow() 
    {
        if (!glfwInit())
        {
            throw std::runtime_error("GLFW could not be initialized.");
        }

        // Because GLFW was originally designed to create an OpenGL context, we need to tell it to not create an OpenGL context.
        glfwWindowHint(GLFW_CLIENT_API, GLFW_NO_API);
        // Handling resized windows takes special care
        //glfwWindowHint(GLFW_RESIZABLE, GLFW_FALSE);

        // The 4th parameter allows you to optionally specify a monitor to open the window on and the last parameter is only relevant to OpenGL.
        window = glfwCreateWindow(WIDTH, HEIGHT, "Vulkan", nullptr, nullptr);

        if (window == nullptr)
        {
            glfwTerminate();

            throw std::runtime_error("Window could not be created.");
        }

        glfwSetWindowUserPointer(window, this); // this value can now be retrieved from within the callback
        glfwSetFramebufferSizeCallback(window, framebufferResizeCallback);
    }

    std::vector<const char*> getRequiredExtensions()
    {
        uint32_t glfwExtensionCount = 0;
        /*
            Returns an array of names of Vulkan instance extensions required by GLFW for creating Vulkan surfaces for GLFW windows.
            If successful, the list will always contain VK_KHR_surface, so if you don't require any
            additional extensions you can pass this list directly to the VkInstanceCreateInfo struct.
        */
        const char** glfwExtensions = glfwGetRequiredInstanceExtensions(&glfwExtensionCount);

        std::vector<const char*> requiredExtensions { glfwExtensions, glfwExtensions + glfwExtensionCount };

        // The extensions specified by GLFW are always required, but the debug messenger extension is conditionally added.
        if (enableValidationLayers)
        {
            requiredExtensions.push_back(VK_EXT_DEBUG_UTILS_EXTENSION_NAME); // Using this macro rather than the literal helps avoid typos.
        }

        return requiredExtensions;
    }

    // Instance extensions add new functionalities that are global to the application and not specific to any particular GPU (device).
    void checkVulkanExtensions(const std::vector<const char*>& requiredExtensions)
    {
        // 1. To allocate an array to hold the extension details, we first need to know how many there are:
        uint32_t extensionCount = 0;
        vkEnumerateInstanceExtensionProperties(nullptr, &extensionCount, nullptr);

        // 2. Allocate an array to hold the extension details. Each VkExtensionProperties struct contains the name and version of an extension.
        std::vector<VkExtensionProperties> vkExtensions{ extensionCount };

        // 3. Query the extension details:
        vkEnumerateInstanceExtensionProperties(nullptr, &extensionCount, vkExtensions.data());

        std::cout << "Available Vulkan extensions:" << std::endl;

        for (const auto& vkExtension : vkExtensions) 
        {
            std::cout << '\t' << vkExtension.extensionName << std::endl;
        }

        std::cout << "Required extensions:" << std::endl;

        for (const auto& extension : requiredExtensions)
        {
            std::cout << '\t' << extension << std::endl;

            auto iterator = std::find_if(
                vkExtensions.begin(),
                vkExtensions.end(),
                [&extension](const VkExtensionProperties& vkExtension) 
                {
                    // strcmp needed for C-style string comparison
                    return strcmp(vkExtension.extensionName, extension) == 0;
                    //return vkExtension.extensionName == extension;
                }
            );

            if (iterator == vkExtensions.end())
            {
                std::ostringstream output;
                output << "Required extension " << extension << " is not supported by Vulkan.";

                throw std::runtime_error(output.str());
            }
        }
    }

    bool checkValidationLayerSupport()
    {
        uint32_t layerCount;
        vkEnumerateInstanceLayerProperties(&layerCount, nullptr);

        std::vector<VkLayerProperties> availableLayers { layerCount };
        vkEnumerateInstanceLayerProperties(&layerCount, availableLayers.data());

        std::cout << "Validation Layers supported by Vulkan:" << std::endl;

        for (const char* layerName : validationLayers) 
        {
            bool layerFound = false;

            for (const auto& layerProperties : availableLayers) 
            {
                std::cout << '\t' << layerProperties.layerName << std::endl;

                if (strcmp(layerName, layerProperties.layerName) == 0) 
                {
                    layerFound = true;
                    break;
                }
            }
            
            if (!layerFound) 
            {
                return false;
            }
        }

        return true;
    }

    /*
        The instance is the connection between your application and the Vulkan library and 
        creating it involves specifying some details about your application to the driver.
    */
    void createInstance()
    {
        if (enableValidationLayers && !checkValidationLayerSupport())
        {
            throw std::runtime_error("Validation layers requested, but not available.");
        }

        /*
            This data is technically optional, but it may provide some useful information to the driver in order to optimize 
            your specific application (e.g. because it uses a well-known graphics engine with certain special behavior).
        */
        VkApplicationInfo appInfo {};

        appInfo.sType = VK_STRUCTURE_TYPE_APPLICATION_INFO;
        appInfo.pApplicationName = "Hello Triangle";
        appInfo.applicationVersion = VK_MAKE_API_VERSION(0, 1, 0, 0);
        appInfo.pEngineName = "No Engine";
        appInfo.engineVersion = VK_MAKE_API_VERSION(0, 1, 0, 0);
        appInfo.apiVersion = VK_API_VERSION_1_3;

        /*
            This is not optional and tells the Vulkan driver which global extensions and validation layers we want to use. 
            Global here means that they apply to the entire program and not a specific device.
        */
        auto extensions = getRequiredExtensions();

        /*
            Per the vkCreateInstance documentation, one of the possible error codes is VK_ERROR_EXTENSION_NOT_PRESENT.
            We could simply specify the extensions we require and terminate if that error code comes back.
            That makes sense for essential extensions like the window system interface, but what if we want to check for optional functionality?
        */
        checkVulkanExtensions(extensions);

        VkInstanceCreateInfo createInfo {};
         
        createInfo.sType = VK_STRUCTURE_TYPE_INSTANCE_CREATE_INFO;
        createInfo.pApplicationInfo = &appInfo;
        /*
            Specify the desired global extensions; since Vulkan is a platform agnostic API, we need an extension to interface with the window 
            system. GLFW has a handy built-in function that returns the extension(s) it needs to do that - which we can pass to the struct.
        */
        createInfo.enabledExtensionCount = static_cast<uint32_t>(extensions.size());
        createInfo.ppEnabledExtensionNames = extensions.data();

        /* 
            The last members determine the global validation layers to enable(for debug mode).

            The vkCreateDebugUtilsMessengerEXT call requires a valid VkInstance to have been created 
            and vkDestroyDebugUtilsMessengerEXT must be called before the VkInstance is destroyed. 
            This leaves us unable to debug any issues in the vkCreateInstance and vkDestroyInstance calls.
            However, per the extension documentation, there is a way to create a separate debug utils messenger 
            specifically for these two function calls. It requires us to pass a pointer to a 
            VkDebugUtilsMessengerCreateInfoEXT struct in the pNext extension field of VkInstanceCreateInfo.

            The debugCreateInfo variable is placed outside the if statement to ensure that it is not destroyed 
            before the vkCreateInstance call. By creating an additional debug messenger this way it will 
            automatically be used during vkCreateInstance and vkDestroyInstance and cleaned up after that.
        */
        VkDebugUtilsMessengerCreateInfoEXT debugCreateInfo {};

        if (enableValidationLayers) 
        {
            createInfo.enabledLayerCount = static_cast<uint32_t>(validationLayers.size());
            createInfo.ppEnabledLayerNames = validationLayers.data();

            populateDebugMessengerCreateInfo(debugCreateInfo);

            createInfo.pNext = (VkDebugUtilsMessengerCreateInfoEXT*)&debugCreateInfo;
        }
        else 
        {
            createInfo.enabledLayerCount = 0;
            createInfo.pNext = nullptr;
        }

        /*
            This is the general pattern followed by object creation function parameters in Vulkan:

            Pointer to struct with creation info.
            Pointer to custom allocator callbacks, always nullptr in this tutorial.
            Pointer to the variable that stores the handle to the new object.
        
            Nearly all Vulkan functions return a value of type VkResult that is either VK_SUCCESS or an error code. 
            To check if the instance was created successfully, we can just check for the success value:
        */
        if (vkCreateInstance(&createInfo, nullptr, &instance) != VK_SUCCESS) 
        {
            throw std::runtime_error("Failed to create VkInstance.");
        }
    }

    void populateDebugMessengerCreateInfo(VkDebugUtilsMessengerCreateInfoEXT& createInfo)
    {
        createInfo = {};
        createInfo.sType = VK_STRUCTURE_TYPE_DEBUG_UTILS_MESSENGER_CREATE_INFO_EXT;
        createInfo.messageSeverity = VK_DEBUG_UTILS_MESSAGE_SEVERITY_VERBOSE_BIT_EXT | VK_DEBUG_UTILS_MESSAGE_SEVERITY_WARNING_BIT_EXT | VK_DEBUG_UTILS_MESSAGE_SEVERITY_ERROR_BIT_EXT;
        createInfo.messageType = VK_DEBUG_UTILS_MESSAGE_TYPE_GENERAL_BIT_EXT | VK_DEBUG_UTILS_MESSAGE_TYPE_VALIDATION_BIT_EXT | VK_DEBUG_UTILS_MESSAGE_TYPE_PERFORMANCE_BIT_EXT;
        createInfo.pfnUserCallback = debugCallback; // pointer to the callback function
        createInfo.pUserData = nullptr; // Optional, such as a pointer to the HelloTriangleApplication class
    }

    void setupDebugMessenger()
    {
        if (!enableValidationLayers) return;

        // Fill in a struct with details about the messenger and its callback.
        VkDebugUtilsMessengerCreateInfoEXT createInfo;

        populateDebugMessengerCreateInfo(createInfo);

        /*
            Since the debug messenger is specific to our Vulkan instance and its layers, it needs to be explicitly specified as first argument.
            You will also see this pattern with other child objects later on.
        */
        if (CreateDebugUtilsMessengerEXT(instance, &createInfo, nullptr, &debugMessenger) != VK_SUCCESS)
        {
            throw std::runtime_error("Failed to set up debug messenger.");
        }
    }

    void createSurface()
    {
        // Under the hood, GLFW performs platform-specific operations (i.e. vkCreateWin32SurfaceKHR)
        if (glfwCreateWindowSurface(instance, window, nullptr, &surface) != VK_SUCCESS)
        {
            throw std::runtime_error("GLFW failed to create window surface.");
        }
    }

    QueueFamilyIndices findQueueFamilies(VkPhysicalDevice device)
    {
        QueueFamilyIndices indices;

        uint32_t queueFamilyCount;
        vkGetPhysicalDeviceQueueFamilyProperties(device, &queueFamilyCount, nullptr);

        std::vector<VkQueueFamilyProperties> queueFamilies { queueFamilyCount };
        vkGetPhysicalDeviceQueueFamilyProperties(device, &queueFamilyCount, queueFamilies.data());

        /*
            The VkQueueFamilyProperties struct contains some details about the queue family, including 
            the type of operations that are supported and the number of queues that can be created based 
            on that family. We need to find at least one queue family that supports VK_QUEUE_GRAPHICS_BIT.
        */

        int i = 0;

        for (const auto& queueFamily : queueFamilies)
        {
            // bitwise AND operation between the queueFlags bitmask and the VK_QUEUE_GRAPHICS_BIT constant
            // We could prefer a physical device that supports drawing and presentation in the same queue for improved performance.
            if (queueFamily.queueFlags & VK_QUEUE_GRAPHICS_BIT && !indices.graphicsFamily.has_value())
            {
                indices.graphicsFamily = i;
            }

            /*
                Since the presentation is a queue-specific feature, the problem is actually about finding 
                a queue family that supports presenting to the surface we created.
            */
            VkBool32 presentSupport = false;
            vkGetPhysicalDeviceSurfaceSupportKHR(device, i, surface, &presentSupport);

            if (presentSupport)
            {
                indices.presentFamily = i; // may or may not be the same as the graphics queue family
            }

            if (indices.isComplete())
            {
                break;
            }

            i++;
        }

        return indices;
    }

    // Just checking if a swap chain is available is not sufficient because it may not be compatible with our window surface.
    SwapChainSupportDetails querySwapChainSupport(VkPhysicalDevice device)
    {
        SwapChainSupportDetails details;

        // All of the support querying functions have these two as first parameters because they are the core components of the swap chain.
        vkGetPhysicalDeviceSurfaceCapabilitiesKHR(device, surface, &details.capabilities);

        uint32_t formatCount;
        vkGetPhysicalDeviceSurfaceFormatsKHR(device, surface, &formatCount, nullptr);

        if (formatCount > 0)
        {
            details.formats.resize(formatCount); // Make sure the vector is resized to hold all the available formats
            vkGetPhysicalDeviceSurfaceFormatsKHR(device, surface, &formatCount, details.formats.data());
        }

        uint32_t presentModeCount;
        vkGetPhysicalDeviceSurfacePresentModesKHR(device, surface, &presentModeCount, nullptr);

        if (presentModeCount > 0)
        {
            details.presentModes.resize(presentModeCount);
            vkGetPhysicalDeviceSurfacePresentModesKHR(device, surface, &presentModeCount, details.presentModes.data());
        }

        return details;
    }

    /*
        Device extensions extend the capabilities of a specific physical device (GPU). 
        Vulkan treats each GPU as a separate device, and device extensions allow you to use 
        additional features or optimizations offered by that GPU beyond what the core Vulkan specification supports.
        Another example of a device extension is VK_NV_ray_tracing, which adds support for hardware-accelerated ray tracing on NVIDIA GPUs.
    */
    bool checkDeviceExtensionSupport(VkPhysicalDevice device)
    {
        uint32_t extensionCount;
        vkEnumerateDeviceExtensionProperties(device, nullptr, &extensionCount, nullptr);

        std::vector<VkExtensionProperties> availableExtensions { extensionCount };
        vkEnumerateDeviceExtensionProperties(device, nullptr, &extensionCount, availableExtensions.data());

        std::set<std::string> requiredExtensions(deviceExtensions.begin(), deviceExtensions.end());

        //std::cout << "Available device extensions:" << std::endl;

        for (const auto& extension : availableExtensions)
        {
            //std::cout << '\t' << extension.extensionName << std::endl;

            requiredExtensions.erase(extension.extensionName);
        }

        /*
            Verify that the graphics card is capable of creating a swap chain. It should be noted that the availability of a
            presentation queue, as we checked in the previous chapter, implies that the swap chain extension must be supported.
            However, it’s still good to be explicit about things, and the extension does have to be explicitly enabled.
        */
        return requiredExtensions.empty();
    }

    /*
        We need to evaluate each GPU and check if they are suitable for the operations we want to perform, 
        because not all graphics cards are created equal.
    */
    bool isDeviceSuitable(VkPhysicalDevice device)
    {
        // Basic device properties like the name, type, and supported Vulkan version can be queried using vkGetPhysicalDeviceProperties:
        VkPhysicalDeviceProperties deviceProperties;
        vkGetPhysicalDeviceProperties(device, &deviceProperties);

        /*
            typedef enum VkPhysicalDeviceType {
                VK_PHYSICAL_DEVICE_TYPE_OTHER = 0,
                VK_PHYSICAL_DEVICE_TYPE_INTEGRATED_GPU = 1,
                VK_PHYSICAL_DEVICE_TYPE_DISCRETE_GPU = 2,
                VK_PHYSICAL_DEVICE_TYPE_VIRTUAL_GPU = 3,
                VK_PHYSICAL_DEVICE_TYPE_CPU = 4,
            } VkPhysicalDeviceType;
        */
        std::cout << deviceProperties.deviceName << " | " << deviceProperties.deviceType << std::endl;

        // Querying support for optional features like texture compression, 64 bit floats, multi viewport rendering (useful for VR), etc.
        VkPhysicalDeviceFeatures deviceFeatures;
        vkGetPhysicalDeviceFeatures(device, &deviceFeatures);

        auto vr = deviceFeatures.multiViewport ? "VR supported" : "VR unsupported";

        std::cout << vr << std::endl;

        QueueFamilyIndices indices = findQueueFamilies(device);

        bool swapChainAdequate = false; // true implies device extensions are supported

        // Only try to query for swap chain support after verifying that the extension is available.
        if (checkDeviceExtensionSupport(device))
        {
            SwapChainSupportDetails swapChainSupport = querySwapChainSupport(device);
            // Checks whether the swap chain is compatible with the window surface
            swapChainAdequate = !swapChainSupport.formats.empty() && !swapChainSupport.presentModes.empty();
        }
        
        // Discrete GPUs have a significant performance advantage.
        return indices.isComplete() && 
               swapChainAdequate && 
               deviceProperties.deviceType == VK_PHYSICAL_DEVICE_TYPE_DISCRETE_GPU &&
               deviceFeatures.samplerAnisotropy;
    }

    /*
        Instead of just checking if a device is suitable or not and going with the first one, you could also give each 
        device a score and pick the highest one. That way you could favor a dedicated graphics card by giving it a 
        higher score, but fall back to an integrated GPU if that’s the only available one.
    */
    void selectPhysicalDevice()
    {
        uint32_t deviceCount = 0;
        vkEnumeratePhysicalDevices(instance, &deviceCount, nullptr);

        if (deviceCount == 0) 
        {
            throw std::runtime_error("Failed to find GPUs with Vulkan support.");
        }

        std::vector<VkPhysicalDevice> devices { deviceCount };
        vkEnumeratePhysicalDevices(instance, &deviceCount, devices.data());

        std::cout << "Found " << deviceCount << " GPU(s) with Vulkan support." << std::endl;

        // We can select any number of graphics cards and use them simultaneously; using only 1 in this tutorial.
        for (const auto& device : devices)
        {
            if (isDeviceSuitable(device))
            {
                physicalDevice = device;
                break;
            }
        }

        if (physicalDevice == VK_NULL_HANDLE)
        {
            throw std::runtime_error("Failed to find a suitable GPU.");
        }
    }

    void createLogicalDevice()
    {
        QueueFamilyIndices indices = findQueueFamilies(physicalDevice);

        std::vector<VkDeviceQueueCreateInfo> queueCreateInfos;
        // Using a set in case it's the same queue family for both capabilities
        std::set<uint32_t> uniqueQueueFamilies = { indices.graphicsFamily.value(), indices.presentFamily.value() };

        float queuePriority = 1.0f;

        for (uint32_t queueFamily : uniqueQueueFamilies)
        {
            VkDeviceQueueCreateInfo queueCreateInfo {};

            queueCreateInfo.sType = VK_STRUCTURE_TYPE_DEVICE_QUEUE_CREATE_INFO;
            queueCreateInfo.queueFamilyIndex = queueFamily;
            /*
                We don’t really need more than one queue per family because we can create all of the command buffers
                on multiple threads and then submit them all at once on the main thread with a single low-overhead call.
            */
            queueCreateInfo.queueCount = 1;
            /*
                Vulkan lets us assign priorities to queues to influence the scheduling of command buffer execution
                using floating point numbers between 0.0 and 1.0. This is required even if there is only a single queue.
            */
            queueCreateInfo.pQueuePriorities = &queuePriority;

            queueCreateInfos.push_back(queueCreateInfo);
        }

        VkPhysicalDeviceFeatures deviceFeatures {};
        // required for texture samplers
        deviceFeatures.samplerAnisotropy = VK_TRUE;

        VkDeviceCreateInfo createInfo {};

        createInfo.sType = VK_STRUCTURE_TYPE_DEVICE_CREATE_INFO;
        createInfo.queueCreateInfoCount = static_cast<uint32_t>(queueCreateInfos.size());
        createInfo.pQueueCreateInfos = queueCreateInfos.data();
        createInfo.pEnabledFeatures = &deviceFeatures;
        /* 
            The remainder of the information is similar to the VkInstanceCreateInfo struct and requires us to
            specify extensions and validation layers. The difference is that these are device specific this time.
            An example of a device specific extension is VK_KHR_swapchain, which allows us to present rendered images 
            from that device to windows. It is possible that there are Vulkan devices in the system that lack this ability, 
            for example because they only support compute operations.
        */
        createInfo.enabledExtensionCount = static_cast<uint32_t>(deviceExtensions.size());
        createInfo.ppEnabledExtensionNames = deviceExtensions.data();

        /*
            Previous implementations of Vulkan made a distinction between instance and device specific validation layers, but 
            this is no longer the case. The enabledLayerCount and ppEnabledLayerNames fields of VkDeviceCreateInfo are ignored 
            by up-to-date implementations. However, it is still a good idea to set them anyway to be compatible with older implementations.
        */
        if (enableValidationLayers)
        {
            createInfo.enabledLayerCount = static_cast<uint32_t>(validationLayers.size());
            createInfo.ppEnabledLayerNames = validationLayers.data();
        }
        else
        {
            createInfo.enabledLayerCount = 0;
        }

        // Similarly to the instance creation function, this call can return errors based on enabling 
        // non-existent extensions or specifying the desired usage of unsupported features.
        if (vkCreateDevice(physicalDevice, &createInfo, nullptr, &device) != VK_SUCCESS)
        {
            throw std::runtime_error("Failed to create logical device.");
        }
        
        // Queue index 0 for both because each queue family only has a single queue.
        // In case the queue families are the same, the two handles will most likely have the same value.
        vkGetDeviceQueue(device, indices.graphicsFamily.value(), 0, &graphicsQueue);
        vkGetDeviceQueue(device, indices.presentFamily.value(), 0, &presentQueue);
    }

    /*
        Each VkSurfaceFormatKHR entry contains a format and a colorSpace member. The format member specifies the color 
        channels and types. For example, VK_FORMAT_B8G8R8A8_SRGB means that we store the B, G, R and alpha channels in 
        that order with an 8 bit unsigned integer for a total of 32 bits per pixel. The colorSpace member indicates if 
        the SRGB color space is supported or not using the VK_COLOR_SPACE_SRGB_NONLINEAR_KHR flag.
    */
    VkSurfaceFormatKHR selectSwapSurfaceFormat(const std::vector< VkSurfaceFormatKHR>& availableFormats)
    {
        for (const auto& availableFormat : availableFormats)
        {
            /*
                For the color space we’ll use SRGB if it is available, because it results in more accurate perceived colors. 
                It is also the standard color space for images, like the textures we’ll use later on. 
                Because of that we should also use an SRGB color format, of which one of the most common ones is VK_FORMAT_B8G8R8A8_SRGB.
            */
            if (availableFormat.format == VK_FORMAT_B8G8R8A8_SRGB && availableFormat.colorSpace == VK_COLOR_SPACE_SRGB_NONLINEAR_KHR)
            {
                return availableFormat;
            }
        }

        // We could start rank the available formats based on how "good" they are, 
        // but in most cases it’s okay to just settle with the first format that is specified.
        return availableFormats[0];
    }

    /*
        The presentation mode is the most important setting for the swap chain because it represents the actual conditions for showing images 
        to the screen. There are four possible modes available in Vulkan:

        VK_PRESENT_MODE_IMMEDIATE_KHR: Images submitted by your application are transferred to the screen right away, which may result in tearing.

        VK_PRESENT_MODE_FIFO_KHR: The swap chain is a queue where the display takes an image from the front of the queue when the display is refreshed and the program inserts rendered images at the back of the queue. If the queue is full then the program has to wait. This is most similar to vertical sync as found in modern games. The moment that the display is refreshed is known as "vertical blank".

        VK_PRESENT_MODE_FIFO_RELAXED_KHR: This mode only differs from the previous one if the application is late and the queue was empty at the last vertical blank. Instead of waiting for the next vertical blank, the image is transferred right away when it finally arrives. This may result in visible tearing.

        VK_PRESENT_MODE_MAILBOX_KHR: This is another variation of the second mode. Instead of blocking the application when the queue is full, the images that are already queued are simply replaced with the newer ones. This mode can be used to render frames as fast as possible while still avoiding tearing, resulting in fewer latency issues than standard vertical sync. This is commonly known as "triple buffering", although the existence of three buffers alone does not necessarily mean that the framerate is unlocked.
    */
    VkPresentModeKHR selectSwapPresentMode(const std::vector<VkPresentModeKHR>& availablePresentModes)
    {
        for (const auto& availablePresentMode : availablePresentModes)
        {
            /*
                VK_PRESENT_MODE_MAILBOX_KHR (triple buffering) is a nice trade-off if energy usage is not a concern. 
                It allows us to avoid tearing while still maintaining a fairly low latency by rendering new images 
                that are as up-to-date as possible right until the vertical blank. 
                On mobile devices, where energy usage is more important, use VK_PRESENT_MODE_FIFO_KHR instead.
            */
            if (availablePresentMode == VK_PRESENT_MODE_MAILBOX_KHR) // ~3,500 FPS per RenderDoc
            {
                return availablePresentMode;
            }
        }

        // VK_PRESENT_MODE_FIFO_KHR (v-sync) mode is guaranteed to be available
        return VK_PRESENT_MODE_FIFO_KHR; // 60 FPS per RenderDoc
    }

    /*
        The swap extent is the resolution of the swap chain images and it’s almost always exactly equal to the resolution of the window 
        that we’re drawing to in pixels. The range of the possible resolutions is defined in the VkSurfaceCapabilitiesKHR structure. 
        Vulkan tells us to match the resolution of the window by setting the width and height in the currentExtent member. 
        However, some window managers do allow us to differ here and this is indicated by setting the width and height in currentExtent 
        to a special value: the maximum value of uint32_t. In that case we’ll pick the resolution that best matches the window within 
        the minImageExtent and maxImageExtent bounds. But we must specify the resolution in the correct unit.
    */
    VkExtent2D selectSwapExtent(const VkSurfaceCapabilitiesKHR& capabilities)
    {
        /*
            GLFW uses two units when measuring sizes: pixels and screen coordinates. For example, the resolution {WIDTH, HEIGHT} 
            that we specified when creating the window is measured in screen coordinates. But Vulkan works with pixels, 
            so the swap chain extent must be specified in pixels as well. Unfortunately, if we are using a high DPI display 
            (like Apple’s Retina display), screen coordinates don’t correspond to pixels. Instead, due to the higher pixel density, 
            the resolution of the window in pixels will be larger than the resolution in screen coordinates. So if Vulkan doesn’t 
            fix the swap extent for us, we can’t just use the original {WIDTH, HEIGHT}. Instead, we must use glfwGetFramebufferSize 
            to query the resolution of the window in pixels before matching it against the minimum and maximum image extent.
        */
        if (capabilities.currentExtent.width == std::numeric_limits<uint32_t>::max())
        {
            // Required by Vulkan since currentExtent width and height do not match the window resolution. A value of 
            // std::numeric_limits<uint32_t>::max() indicates that the surface size will be determined by the extent of the swap chain images.
            int width, height;
            /*
                Retrieves the resolution of the window in pixels. GLFW, or any windowing system, may allow the creation of windows with 
                dimensions outside the bounds supported by the Vulkan implementation on the device for a given surface (i.e. a window might 
                be resized by the user or the system to dimensions larger or smaller than what the device can handle for rendering).
            */
            glfwGetFramebufferSize(window, &width, &height);

            VkExtent2D actualExtent
            {
                static_cast<uint32_t>(width),
                static_cast<uint32_t>(height)
            };

            /*
                The clamp function is used here to bound the values of width and height between the allowed minimum and maximum extents 
                that are supported by the implementation. Clamping is a way to gracefully handle cases where the window size doesn't match 
                the GPU's supported sizes exactly, allowing for flexible window management while still adhering to the device's constraints.
            */
            actualExtent.width = std::clamp(actualExtent.width, capabilities.minImageExtent.width, capabilities.maxImageExtent.width);
            actualExtent.height = std::clamp(actualExtent.height, capabilities.minImageExtent.height, capabilities.maxImageExtent.height);

            return actualExtent;
        }
        else
        {
            // currentExtent width and height already match the window resolution
            return capabilities.currentExtent;
        }
    }

    /*
        Find the settings for the best possible swap chain. If the swapChainAdequate conditions 
        were met then the support is definitely sufficient, but there may still be many 
        different modes of varying optimality. There are three types of settings to determine:

        1. Surface format (color depth)
        2. Presentation mode (conditions for "swapping" images to the screen)
        3. Swap extent (resolution of images in swap chain)

        For each of these settings we’ll have an ideal value in mind that we’ll go with if 
        it’s available and otherwise we’ll create some logic to find the next best thing.
    */
    void createSwapChain()
    {
        SwapChainSupportDetails swapChainSupport = querySwapChainSupport(physicalDevice);

        VkSurfaceFormatKHR surfaceFormat = selectSwapSurfaceFormat(swapChainSupport.formats);
        VkPresentModeKHR presentMode = selectSwapPresentMode(swapChainSupport.presentModes);
        VkExtent2D extent = selectSwapExtent(swapChainSupport.capabilities);

        /*
            Decide how many images we would like to have in the swap chain. Selecting the minimum means that we may sometimes have 
            to wait on the driver to complete internal operations before we can acquire another image to render to. Therefore it is 
            recommended to request at least one more image than the minimum. We should also make sure to not exceed the maximum 
            number of images, where 0 is a special value that means that there is no maximum.
        */
        uint32_t imageCount = swapChainSupport.capabilities.minImageCount + 1;

        if (swapChainSupport.capabilities.maxImageCount > 0 && imageCount > swapChainSupport.capabilities.maxImageCount) 
        {
            imageCount = swapChainSupport.capabilities.maxImageCount;
        }

        VkSwapchainCreateInfoKHR createInfo {};

        createInfo.sType = VK_STRUCTURE_TYPE_SWAPCHAIN_CREATE_INFO_KHR;
        createInfo.surface = surface;
        createInfo.minImageCount = imageCount;
        createInfo.imageFormat = surfaceFormat.format;
        createInfo.imageColorSpace = surfaceFormat.colorSpace;
        createInfo.imageExtent = extent;
        /*
            Single Layer: For the majority of applications, each image in the swap chain will consist of a single layer. 
            This is the case for standard 2D applications, where you're rendering a single image to be displayed on the screen. 
            Therefore, imageArrayLayers is always set to 1.

            Stereoscopic 3D Applications: In the context of stereoscopic 3D applications, the application needs to render a separate image 
            for each eye to create a 3D effect. This requires two layers per image in the swap chain—one for the left eye and one for the 
            right eye. For such applications, imageArrayLayers would be set to 2 to accommodate both layers within each swap chain image.
            Vulkan supports not only standard 2D rendering but also more complex scenarios like stereoscopic 3D, virtual reality (VR), 
            and augmented reality (AR), where multiple views (layers) are required to be rendered and presented simultaneously.
        */
        createInfo.imageArrayLayers = 1;
        /*
            We are going to render directly to them, which means that they’re used as color attachment. It is also possible to render 
            images to a separate image first to perform operations like post-processing. In that case you may use a value like 
            VK_IMAGE_USAGE_TRANSFER_DST_BIT instead and use a memory operation to transfer the rendered image to a swap chain image.
        */
        createInfo.imageUsage = VK_IMAGE_USAGE_COLOR_ATTACHMENT_BIT;

        QueueFamilyIndices indices = findQueueFamilies(physicalDevice);
        uint32_t queueFamilyIndices[] = { indices.graphicsFamily.value(), indices.presentFamily.value() };

        if (indices.graphicsFamily != indices.presentFamily) 
        {
            // Images can be used across multiple queue families without explicit ownership transfers.
            createInfo.imageSharingMode = VK_SHARING_MODE_CONCURRENT;
            createInfo.queueFamilyIndexCount = 2;
            createInfo.pQueueFamilyIndices = queueFamilyIndices;
        }
        else 
        {
            /*
                An image is owned by one queue family at a time and ownership must be explicitly transferred 
                before using it in another queue family. This option offers the best performance.
                The graphics queue family and presentation queue family are the same on most GPUs.
            */
            createInfo.imageSharingMode = VK_SHARING_MODE_EXCLUSIVE;
            createInfo.queueFamilyIndexCount = 0; // Optional
            createInfo.pQueueFamilyIndices = nullptr; // Optional
        }

        /*
            We can specify that a certain transform should be applied to images in the swap chain if it is supported 
            (supportedTransforms in capabilities), like a 90 degree clockwise rotation or horizontal flip. 
            To specify that you do not want any transformation, simply specify the current transformation.
        */
        createInfo.preTransform = swapChainSupport.capabilities.currentTransform;
        /*
            specifies if the alpha channel should be used for blending with other windows in the window system.
            We almost always want to simply ignore the alpha channel, hence VK_COMPOSITE_ALPHA_OPAQUE_BIT_KHR.
        */ 
        createInfo.compositeAlpha = VK_COMPOSITE_ALPHA_OPAQUE_BIT_KHR;
        createInfo.presentMode = presentMode;
        /*
            If clipped is set to VK_TRUE then that means that we don’t care about the color of pixels that are obscured, 
            for example because another window is in front of them. Unless you really need to be able to read these 
            pixels back and get predictable results, you’ll get the best performance by enabling clipping.
        */
        createInfo.clipped = VK_TRUE;
        /*
            With Vulkan it’s possible that your swap chain becomes invalid or unoptimized while your application is running, 
            for example because the window was resized. In that case the swap chain actually needs to be recreated from scratch and a 
            reference to the old one must be specified in this field. For now we assume that we will only ever create one swap chain.
        */
        createInfo.oldSwapchain = VK_NULL_HANDLE;

        if (vkCreateSwapchainKHR(device, &createInfo, nullptr, &swapChain) != VK_SUCCESS)
        {
            throw std::runtime_error("Failed to create swap chain.");
        }

        // We now have a set of images that can be drawn onto and can be presented to the window.

        // We will reference swap chain images during rendering operations later
        vkGetSwapchainImagesKHR(device, swapChain, &imageCount, nullptr);
        swapChainImages.resize(imageCount);
        vkGetSwapchainImagesKHR(device, swapChain, &imageCount, swapChainImages.data());
        // These will be needed later
        swapChainImageFormat = surfaceFormat.format;
        swapChainExtent = extent;
    }

    VkImageView createImageView(VkImage image, VkFormat format, VkImageAspectFlags aspectFlags, uint32_t mipLevels)
    {
        VkImageViewCreateInfo viewInfo {};

        viewInfo.sType = VK_STRUCTURE_TYPE_IMAGE_VIEW_CREATE_INFO;
        viewInfo.image = image;
        // The viewType and format fields specify how the image data should be interpreted. 
        // The viewType parameter allows you to treat images as 1D textures, 2D textures, 3D textures and cube maps.
        viewInfo.format = format;
        viewInfo.viewType = VK_IMAGE_VIEW_TYPE_2D;
        /*
            The subresourceRange field describes what is the purpose of the image and which part of the image should be accessed.
            Our images will be used as color targets without multiple layers.
            If we were working on a stereographic 3D application, then we would create a swap chain with multiple layers. We could then
            create multiple image views for each image representing the views for the left and right eyes by accessing different layers.

            Mipmaps are precalculated, downscaled versions of an image. Each new image is half the width and height of the previous one. 
            Mipmaps are used as a form of Level of Detail or LOD. Objects that are far away from the camera will sample their textures from 
            the smaller mip images. Using smaller images increases the rendering speed and avoids artifacts such as Moiré patterns.
            In Vulkan, each of the mip images is stored in different mip levels of a VkImage. Mip level 0 is the original image, 
            and the mip levels after level 0 are commonly referred to as the mip chain.
        */
        viewInfo.subresourceRange.aspectMask = aspectFlags;
        viewInfo.subresourceRange.baseMipLevel = 0;
        viewInfo.subresourceRange.levelCount = mipLevels;
        viewInfo.subresourceRange.baseArrayLayer = 0;
        viewInfo.subresourceRange.layerCount = 1;
        /*
            The components field allows US to swizzle the color channels around.
            For example, you can map all of the channels to the red channel for a monochrome texture.
            We can also map constant values of 0 and 1 to a channel. In our case we’ll stick to the default mapping.
        */
        viewInfo.components.r = VK_COMPONENT_SWIZZLE_IDENTITY;
        viewInfo.components.g = VK_COMPONENT_SWIZZLE_IDENTITY;
        viewInfo.components.b = VK_COMPONENT_SWIZZLE_IDENTITY;
        viewInfo.components.a = VK_COMPONENT_SWIZZLE_IDENTITY;

        VkImageView imageView;

        if (vkCreateImageView(device, &viewInfo, nullptr, &imageView) != VK_SUCCESS)
        {
            throw std::runtime_error("Failed to create image view.");
        }

        return imageView;
    }

    // Creates a basic image view for every image in the swap chain so that we can use them as color targets.
    void createImageViews()
    {
        swapChainImageViews.resize(swapChainImages.size());

        for (size_t i = 0; i < swapChainImages.size(); i++)
        {
            swapChainImageViews[i] = createImageView(swapChainImages[i], swapChainImageFormat, VK_IMAGE_ASPECT_COLOR_BIT, 1);
        }
    }

    /*
        Render pass: The attachments referenced by the pipeline stages and their usage.
        A single render pass can consist of multiple subpasses. Subpasses are subsequent rendering operations that depend on the contents 
        of framebuffers in previous passes, for example a sequence of post-processing effects that are applied one after another. If you 
        group these rendering operations into one render pass, then Vulkan is able to reorder the operations and conserve memory 
        bandwidth for possibly better performance. Every subpass references one or more of the attachments. 
        For a simple triangle, we will stick to a single subpass.
    */
    void createRenderPass()
    {
        // Here we have just a single color buffer attachment represented by one of the images from the swap chain.
        VkAttachmentDescription colorAttachment {};
        // The format of the color attachment should match the format of the swap chain images.
        colorAttachment.format = swapChainImageFormat;
        // We are not doing anything with multisampling yet.
        colorAttachment.samples = VK_SAMPLE_COUNT_1_BIT;
        /*
            The loadOp and storeOp determine what to do with the data in the attachment before rendering and after rendering. 

            We have the following choices for loadOp:

            VK_ATTACHMENT_LOAD_OP_LOAD: Preserve the existing contents of the attachment

            VK_ATTACHMENT_LOAD_OP_CLEAR: Clear the values to a constant at the start

            VK_ATTACHMENT_LOAD_OP_DONT_CARE: Existing contents are undefined; we don’t care about them

            In our case we’re going to use the clear operation to clear the framebuffer to black before drawing a new frame. 
            
            There are only two possibilities for the storeOp:

            VK_ATTACHMENT_STORE_OP_STORE: Rendered contents will be stored in memory and can be read later

            VK_ATTACHMENT_STORE_OP_DONT_CARE: Contents of the framebuffer will be undefined after the rendering operation
        */
        colorAttachment.loadOp = VK_ATTACHMENT_LOAD_OP_CLEAR; // clear the framebuffer to black before drawing a new frame
        colorAttachment.storeOp = VK_ATTACHMENT_STORE_OP_STORE; // store in memory after rendering
        // loadOp and storeOp apply to color and depth data, and stencilLoadOp / stencilStoreOp apply to stencil data
        colorAttachment.stencilLoadOp = VK_ATTACHMENT_LOAD_OP_DONT_CARE; // Our application won’t do anything with the stencil buffer.
        colorAttachment.stencilStoreOp = VK_ATTACHMENT_STORE_OP_DONT_CARE; // Our application won’t do anything with the stencil buffer.
        /*
            Textures and framebuffers in Vulkan are represented by VkImage objects with a certain pixel format.
            However, the layout of the pixels in memory can change based on what you’re trying to do with an image.

            Some of the most common layouts are:

            VK_IMAGE_LAYOUT_COLOR_ATTACHMENT_OPTIMAL: Images used as color attachment

            VK_IMAGE_LAYOUT_PRESENT_SRC_KHR: Images to be presented in the swap chain

            VK_IMAGE_LAYOUT_TRANSFER_DST_OPTIMAL: Images to be used as destination for a memory copy operation (neural rendering?)

            Images need to be transitioned to specific layouts that are suitable for the operation they’re going to be involved in next.

            Using VK_IMAGE_LAYOUT_UNDEFINED for initialLayout means that we don’t care what previous layout the image was in. 
            The caveat of this special value is that the contents of the image are not guaranteed to be preserved, but that 
            doesn’t matter since we’re going to clear it anyway. We want the image to be ready for presentation using the 
            swap chain after rendering, which is why we use VK_IMAGE_LAYOUT_PRESENT_SRC_KHR as finalLayout.
        */
        colorAttachment.initialLayout = VK_IMAGE_LAYOUT_UNDEFINED; // which layout the image will have before the render pass begins
        colorAttachment.finalLayout = VK_IMAGE_LAYOUT_PRESENT_SRC_KHR; // layout to automatically transition to when the render pass finishes

        // Each attachment needs a reference [id]
        VkAttachmentReference colorAttachmentRef {};
        /*
            The attachment parameter specifies which attachment to reference by its index in the attachment descriptions array. 
            Our array starts with this VkAttachmentDescription, so its index is 0. The layout specifies which layout we would 
            like the attachment to have during a subpass that uses this reference. Vulkan will automatically transition the 
            attachment to this layout when the subpass is started. We intend to use the attachment to function as a color 
            buffer and the VK_IMAGE_LAYOUT_COLOR_ATTACHMENT_OPTIMAL layout will give us the best performance.
        */
        colorAttachmentRef.attachment = 0;
        colorAttachmentRef.layout = VK_IMAGE_LAYOUT_COLOR_ATTACHMENT_OPTIMAL;

        VkAttachmentDescription depthAttachment {};

        depthAttachment.format = findDepthFormat(); // format should be the same as the depth image
        depthAttachment.samples = VK_SAMPLE_COUNT_1_BIT;
        depthAttachment.loadOp = VK_ATTACHMENT_LOAD_OP_CLEAR;
        depthAttachment.storeOp = VK_ATTACHMENT_STORE_OP_DONT_CARE; // depth data will not be used after drawing has finished
        depthAttachment.stencilLoadOp = VK_ATTACHMENT_LOAD_OP_DONT_CARE;
        depthAttachment.stencilStoreOp = VK_ATTACHMENT_STORE_OP_DONT_CARE;
        depthAttachment.initialLayout = VK_IMAGE_LAYOUT_UNDEFINED; // like the color buffer, we don’t care about the previous depth contents
        depthAttachment.finalLayout = VK_IMAGE_LAYOUT_DEPTH_STENCIL_ATTACHMENT_OPTIMAL;

        VkAttachmentReference depthAttachmentRef {};
        /*
            The attachment parameter specifies which attachment to reference by its index in the attachment descriptions array.
            The layout specifies which layout we would like the attachment to have during a subpass that uses this reference. 
            Vulkan will automatically transition the attachment to this layout when the subpass is started. We intend to use the attachment 
            to function as a depth buffer and the VK_IMAGE_LAYOUT_DEPTH_STENCIL_ATTACHMENT_OPTIMAL layout will give us the best performance.
        */
        depthAttachmentRef.attachment = 1;
        depthAttachmentRef.layout = VK_IMAGE_LAYOUT_DEPTH_STENCIL_ATTACHMENT_OPTIMAL;

        VkSubpassDescription subpass {};
        // Vulkan may also support compute subpasses in the future, so we have to be explicit about this being a graphics subpass.
        subpass.pipelineBindPoint = VK_PIPELINE_BIND_POINT_GRAPHICS;
        subpass.colorAttachmentCount = 1;
        // The index of the attachment in this array is directly referenced from the fragment shader 
        // with the layout(location = 0) out vec4 outColor directive.
        subpass.pColorAttachments = &colorAttachmentRef; // a subpass can use multiple color attachments
        /*
            The following other types of attachments can be referenced by a subpass:

            pInputAttachments: Attachments that are read from a shader
            pResolveAttachments: Attachments used for multisampling color attachments
            pDepthStencilAttachment: Attachment for depth and stencil data
            pPreserveAttachments: Attachments that are not used by this subpass, but for which the data must be preserved

            There is only one depth value and one stencil value per fragment in a standard rendering pipeline. 
            The depth value represents the distance from the camera, and the stencil value is used for operations like masking. 
            Both are used for depth testing, stencil testing, and depth write operations, which are singular operations per fragment.
            The rasterizer produces fragments (potential pixels) for all color attachments, and then depth testing is performed for all at once.
        */
        subpass.pDepthStencilAttachment = &depthAttachmentRef; // a subpass can only use a single depth (+stencil) attachment

        /*
            Subpasses in a render pass automatically take care of image layout transitions. These transitions are controlled by subpass 
            dependencies, which specify memory and execution dependencies between subpasses. We have only a single subpass right now, 
            but the operations right before and right after this subpass also count as implicit "subpasses".

            There are two built-in dependencies that take care of the transition at the start of the render pass and at the end of the 
            render pass, but the former does not occur at the right time. It assumes that the transition occurs at the start of the pipeline, 
            but we haven’t acquired the image yet at that point. There are two ways to deal with this problem. We could change the 
            waitStages for the imageAvailableSemaphore to VK_PIPELINE_STAGE_TOP_OF_PIPE_BIT to ensure that the render passes don’t begin 
            until the image is available, or we can make the render pass wait for the VK_PIPELINE_STAGE_COLOR_ATTACHMENT_OUTPUT_BIT stage.

            We need to make sure that there is no conflict between the transitioning of the depth image and it being cleared as 
            part of its load operation. The depth image is first accessed in the early fragment test pipeline stage and because 
            we have a load operation that clears, we should specify the access mask for writes.
        */
        VkSubpassDependency dependency {};
        /*
            The first two fields specify the indices of the dependency and the dependent subpass. The special value VK_SUBPASS_EXTERNAL 
            refers to the implicit subpass before or after the render pass depending on whether it is specified in srcSubpass or dstSubpass. 
            The index 0 refers to our subpass, which is the first and only one. The dstSubpass must always be higher than srcSubpass to 
            prevent cycles in the dependency graph (unless one of the subpasses is VK_SUBPASS_EXTERNAL).
        */
        dependency.srcSubpass = VK_SUBPASS_EXTERNAL; // implicit subpass (transition) before the render pass
        dependency.dstSubpass = 0; // our actual subpass (first & only one), which is higher than the implicit subpass before the render pass
        /*
            These specify the operations to wait on and the stages in which these operations occur. We need to wait for the swap chain to 
            finish reading from the image before we can access it. This can be accomplished by waiting on the color attachment output stage.
            srcStageMask and dstStageMask: pipeline stage(s) where the dependency applies
            
            srcStageMask and dstStageMask specify the stages of the pipeline that must wait on this dependency and the stages that are 
            allowed to begin once the dependency conditions are met, respectively. Here, the previous subpass must complete using the
            color attachment and complete using the depth attachment before the current subpass can begin using the attachments.

            srcAccessMask and dstAccessMask specify the types of operations that must be completed (by the source) before the subpass can 
            begin and the types of operations that will be performed in the subpass (destination). Including depth-stencil writes in both 
            source and destination access masks directly relates to the use of a shared depth buffer, ensuring that depth writes from one 
            frame are completed before the next frame begins depth testing or writes.
        */
        dependency.srcStageMask = VK_PIPELINE_STAGE_COLOR_ATTACHMENT_OUTPUT_BIT | VK_PIPELINE_STAGE_LATE_FRAGMENT_TESTS_BIT;
        // the dependency is waiting on this specific type of operation to complete
        dependency.srcAccessMask = VK_ACCESS_DEPTH_STENCIL_ATTACHMENT_WRITE_BIT;
        /*
            The operations that should wait on this are in the color attachment stage and involve the writing of the color attachment. 
            These settings will prevent the transition from happening until it’s actually necessary (and allowed): 
            when we want to start writing colors to it.
        */
        dependency.dstStageMask = VK_PIPELINE_STAGE_COLOR_ATTACHMENT_OUTPUT_BIT | VK_PIPELINE_STAGE_EARLY_FRAGMENT_TESTS_BIT;
        // Specifies that the destination subpass will perform write operations to a color attachment, 
        // and these writes must wait for the dependency to be satisfied.
        dependency.dstAccessMask = VK_ACCESS_COLOR_ATTACHMENT_WRITE_BIT | VK_ACCESS_DEPTH_STENCIL_ATTACHMENT_WRITE_BIT;

        std::array<VkAttachmentDescription, 2> attachments = { colorAttachment, depthAttachment };

        // Fill in with an array of attachments and subpasses
        VkRenderPassCreateInfo renderPassInfo {};

        renderPassInfo.sType = VK_STRUCTURE_TYPE_RENDER_PASS_CREATE_INFO;
        renderPassInfo.attachmentCount = static_cast<uint32_t>(attachments.size());
        renderPassInfo.pAttachments = attachments.data();
        renderPassInfo.subpassCount = 1;
        renderPassInfo.pSubpasses = &subpass;
        renderPassInfo.dependencyCount = 1;
        renderPassInfo.pDependencies = &dependency;

        if (vkCreateRenderPass(device, &renderPassInfo, nullptr, &renderPass) != VK_SUCCESS)
        {
            throw std::runtime_error("Failed to create render pass.");
        }
    }

    /*
        An OBJ file consists of positions, normals, texture coordinates, and faces. Faces consist of an arbitrary amount of vertices, 
        where each vertex refers to a position, normal, and/or texture coordinate by index. This makes it possible to not just reuse 
        entire vertices, but also individual attributes.

        The attrib container holds all of the positions, normals and texture coordinates in its attrib.vertices, attrib.normals, 
        and attrib.texcoords vectors. The shapes container contains all of the separate objects and their faces. Each face consists 
        of an array of vertices, and each vertex contains the indices of the position, normal and texture coordinate attributes. 
        OBJ models can also define a material and texture per face, but we will be ignoring those.

        The err string contains errors and the warn string contains warnings that occurred while loading the file, like a missing material 
        definition. Loading only really failed if the LoadObj function returns false. Faces in OBJ files can contain an arbitrary number of 
        vertices, whereas our application can only render triangles. Luckily the LoadObj has an optional parameter to automatically 
        triangulate such faces, which is enabled by default.
    */
    void loadModel()
    {
        tinyobj::attrib_t attribute;
        std::vector<tinyobj::shape_t> shapes;
        std::vector<tinyobj::material_t> materials;
        std::string warning, error;

        if (!tinyobj::LoadObj(&attribute, &shapes, &materials, &warning, &error, MODEL_PATH.c_str()))
        {
            throw std::runtime_error(warning + error);
        }

        // using a user-defined type (Vertex struct) as key in a hash table requires us to implement two functions: 
        // equality test and hash calculation
        std::unordered_map<Vertex, uint32_t> uniqueVertices {};
        /*
            Combine all of the faces in the file into a single model. The triangulation feature has already made sure that there are 
            three vertices per face, so we can now directly iterate over the vertices and dump them straight into our vertices vector:
        */
        for (const auto& shape : shapes)
        {
            // assuming that every vertex is unique, hence the simple auto-increment indices
            for (const auto& index : shape.mesh.indices)
            {
                /*
                    The index variable contains the vertex_index, normal_index, and texcoord_index members. 
                    We need to use these indices to look up the actual vertex attributes in the attribute arrays.
                    The attribute.vertices array is an array of float values instead of something like glm::vec3, so we need to multiply 
                    the index by 3. Similarly, there are two texture coordinate components per entry. The offsets of 0, 1 and 2 are used 
                    to access the X, Y and Z components, or the U and V components in the case of texture coordinates.
                */
                Vertex vertex {};

                vertex.position = 
                {
                    attribute.vertices[3 * index.vertex_index + 0],
                    attribute.vertices[3 * index.vertex_index + 1],
                    attribute.vertices[3 * index.vertex_index + 2]
                };

                /*
                    The OBJ format assumes a coordinate system where a vertical coordinate of 0 means the bottom of the image, 
                    however we’ve uploaded our image into Vulkan in a top to bottom orientation where 0 means the top of the image. 
                    Solving this by flipping the vertical component of the texture coordinates:
                */
                vertex.texture = 
                {
                    attribute.texcoords[2 * index.texcoord_index + 0],
                    1.0f - attribute.texcoords[2 * index.texcoord_index + 1]
                };

                vertex.color = { 1.0f, 1.0f, 1.0f };

                /*
                    The vertices vector contained a lot of duplicated vertex data because many vertices are included in multiple triangles. 
                    Let's keep only the unique vertices and use the index buffer to reuse them whenever they come up. A straightforward 
                    way to implement this is to use an unordered_map to keep track of the unique vertices and respective indices.
                    Every time we read a vertex from the OBJ file, we check if we’ve already seen a vertex with the exact same position and 
                    texture coordinates before. If not, we add it to vertices and store its index in the uniqueVertices container. After 
                    that we add the index of the new vertex to indices. If we’ve seen the exact same vertex before, then we look up its 
                    index in uniqueVertices and store that index in indices.

                    If we check the size of vertices, we see that it has shrunk down from 1,500,000 to 265,645. 
                    That means that each vertex is reused in an average number of ~6 triangles. 
                    This definitely saves us a lot of GPU memory.
                */
                if (uniqueVertices.count(vertex) == 0) 
                {
                    uniqueVertices[vertex] = static_cast<uint32_t>(vertices.size());

                    vertices.push_back(vertex);
                }

                // indices do repeat; that's the whole point of an index buffer
                indices.push_back(uniqueVertices[vertex]);
            }
        }
    }

    /*
        Graphics cards can offer different types of memory to allocate from. Each type of memory varies in 
        terms of allowed operations and performance characteristics. We need to combine the requirements 
        of the buffer and our own application requirements to find the right type of memory to use.
        The typeFilter parameter is a bitmask representing which memory types are viable options. 
        The properties parameter specifies the desired properties of the memory type (like being device local, host visible, etc.).
    */
    uint32_t findMemoryType(uint32_t typeFilter, VkMemoryPropertyFlags properties)
    {
        VkPhysicalDeviceMemoryProperties memoryProperties;

        vkGetPhysicalDeviceMemoryProperties(physicalDevice, &memoryProperties);

        /*
            The VkPhysicalDeviceMemoryProperties structure has two arrays memoryTypes and memoryHeaps. 
            Memory heaps are distinct memory resources like dedicated VRAM and swap space in RAM for when VRAM runs out. 
            The different types of memory exist within these heaps. Right now we’ll only concern ourselves 
            with the type of memory and not the heap it comes from, but this can affect performance.
        */
        for (uint32_t i = 0; i < memoryProperties.memoryTypeCount; i++)
        {
            /*
                Find a memory type that is suitable for the [vertex] buffer. We also need to be able to write our data to that memory. 
                The memoryTypes array consists of VkMemoryType structs that specify the heap and properties of each type of memory. 
                The properties define special features of the memory, like being able to map it so we can write to it from the CPU. 
                This property is indicated with VK_MEMORY_PROPERTY_HOST_VISIBLE_BIT, 
                but we also need to use the VK_MEMORY_PROPERTY_HOST_COHERENT_BIT property.
             
                Bitwise AND (&): 
                    This operation compares each bit of its first operand to the corresponding bit of its second operand. 
                    If both bits are 1, the resulting bit is 1; otherwise, it is 0.
                Left Shift (1 << i): 
                    This operation takes the number 1 and shifts it left by i bits, effectively creating a 
                    bitmask where the ith bit is set to 1, and all other bits are 0.
            */
            if ((typeFilter & (1 << i)) && (memoryProperties.memoryTypes[i].propertyFlags & properties) == properties)
            {
                return i;
            }
        }
        /*
            We may have more than one desirable property, so we should check if the result of the bitwise AND is not just non-zero, 
            but equal to the desired properties bit field. If there is a memory type suitable for the buffer that also has all of 
            the properties we need, then we return its index, otherwise we throw an exception.
        */
        throw std::runtime_error("Failed to find a suitable memory type.");
    }

    /*
        Separating the description of the data structure (VkBuffer) from the actual memory allocation (VkDeviceMemory)
        allows for more efficient memory usage. Multiple buffers can be bound to different regions of the same
        VkDeviceMemory allocation, reducing the overhead and fragmentation of memory allocations.
        This is particularly useful for managing many small resources, such as uniform buffers.
        Buffers are used to organize data in a way that is accessible by the GPU, but they do not themselves allocate memory.
        Instead, they define the structure, usage, and properties of the data storage, such as whether it will be used for
        vertex input, as a storage buffer, or for other purposes.
    */
    void createBuffer(
        VkDeviceSize size, 
        VkBufferUsageFlags usage, 
        VkMemoryPropertyFlags properties, 
        VkBuffer& buffer, 
        VkDeviceMemory& bufferMemory
    ) {
        VkBufferCreateInfo bufferInfo {};
        // queueFamilyIndexCount and pQueueFamilyIndices are ignored if sharingMode != VK_SHARING_MODE_CONCURRENT
        bufferInfo.sType = VK_STRUCTURE_TYPE_BUFFER_CREATE_INFO;
        bufferInfo.size = size;
        bufferInfo.usage = usage;
        bufferInfo.sharingMode = VK_SHARING_MODE_EXCLUSIVE;

        if (vkCreateBuffer(device, &bufferInfo, nullptr, &buffer) != VK_SUCCESS) 
        {
            throw std::runtime_error("Failed to create buffer.");
        }

        /*
            size: The size of the required amount of memory in bytes; may differ from bufferInfo.size.
            alignment: The offset in bytes where the buffer begins in the allocated region of memory; depends on bufferInfo.usage and bufferInfo.flags.
            memoryTypeBits: Bit field of the memory types that are suitable for the buffer.
        */
        VkMemoryRequirements memoryRequirements;
        vkGetBufferMemoryRequirements(device, buffer, &memoryRequirements);

        VkMemoryAllocateInfo allocInfo{};

        allocInfo.sType = VK_STRUCTURE_TYPE_MEMORY_ALLOCATE_INFO;
        allocInfo.allocationSize = memoryRequirements.size;
        /*
            When a GPU memory heap is flagged as host coherent (VK_MEMORY_PROPERTY_HOST_COHERENT_BIT), it indicates a specific behavior regarding the coherency between memory accesses by the CPU (host) and the GPU (device). This property affects how memory writes made by the host are visible to the device and vice versa, especially in the context of mapped memory regions. Here’s what it entails:

            Coherency Without Explicit Flushes or Invalidates
            Automatic Visibility: Changes made to a mapped memory region on the CPU side are automatically visible to the GPU, and changes made by the GPU are automatically visible to the CPU. This means that, under normal circumstances, you do not need to explicitly flush changes from the host to the device or invalidate them on the host to see changes made by the device.
            Synchronization Still Required: While host coherency ensures visibility of the writes across the CPU and GPU, it does not eliminate the need for proper synchronization to manage the order of operations across the CPU and GPU. For instance, you still need to use fences, semaphores, or barriers to ensure that the GPU has finished reading or writing to a memory region before the CPU starts its operations, or vice versa.
            Impact on Performance and Usage
            Performance Considerations: Although host coherency simplifies some aspects of memory management by removing the need for explicit flushes and invalidations, it may come with performance implications. The automatic maintenance of coherency can introduce overheads, as the hardware or driver might need to perform additional operations to ensure that memory views are consistent between the host and the device.
            Usage Scenario: Host coherent memory is particularly useful for scenarios where the application frequently updates data that the GPU consumes, such as dynamic uniform buffers or frequently updated vertex buffers. It simplifies code by removing the need for explicit memory management calls to maintain visibility.
            Understanding the Trade-offs
            Using host coherent memory effectively requires understanding the trade-offs between ease of use and potential performance impacts. For applications with intensive memory update patterns and synchronization requirements, the benefits of host coherency in simplifying development might outweigh the performance considerations. However, for performance-critical applications, it's important to profile and test different memory properties to find the optimal balance between performance and programming convenience.

            In summary, VK_MEMORY_PROPERTY_HOST_COHERENT_BIT simplifies memory management between the CPU and GPU by ensuring automatic visibility of memory operations without the need for explicit flush or invalidate commands. However, developers must still handle synchronization explicitly and consider the potential performance impacts of using host coherent memory.

            vkFlushMappedMemoryRanges:
            When you write to a mapped memory region on the host, those writes might reside in the CPU's cache and not be immediately visible to the device. vkFlushMappedMemoryRanges is used to ensure that any writes made to the mapped memory regions are flushed from the host cache and made visible to the device. This operation is crucial for memory regions without the VK_MEMORY_PROPERTY_HOST_COHERENT_BIT because, without it, there's no guarantee that the device will see the updated data when it tries to access the memory.
            Usage: Call vkFlushMappedMemoryRanges after writing to a mapped memory region and before any device access (such as GPU read operations) to ensure that the writes are visible to the device.

            vkInvalidateMappedMemoryRanges:
            Conversely, when the device writes to memory that the host plans to read, those writes may not be immediately visible to the host due to similar coherency issues. vkInvalidateMappedMemoryRanges is used to invalidate any cached data in the mapped memory regions on the host, ensuring that the host reads the most recent data written by the device. This operation is necessary for memory regions that do not automatically maintain coherency between host and device accesses.
            Usage: Call vkInvalidateMappedMemoryRanges before reading from a mapped memory region on the host if the device has written to that memory, to ensure that the host sees the latest changes made by the device.
        */
        allocInfo.memoryTypeIndex = findMemoryType(memoryRequirements.memoryTypeBits, properties);

        /*
            In a real world application we are not supposed to call vkAllocateMemory for every individual buffer. The maximum number of 
            simultaneous memory allocations is limited by the maxMemoryAllocationCount physical device limit, which may be as low as 4096 
            even on high end hardware like an NVIDIA GTX 1080. The right way to allocate memory for a large number of objects at the same 
            time is to create a custom allocator that splits up a single allocation among many different objects by using the offset 
            parameters that we’ve seen in many functions. We can either implement such an allocator ourselves, or use the 
            VulkanMemoryAllocator library provided by the GPUOpen initiative. However, for this tutorial it’s okay to use 
            a separate allocation for every resource because we won’t come close to hitting any of these limits for now.
        */
        if (vkAllocateMemory(device, &allocInfo, nullptr, &bufferMemory) != VK_SUCCESS) 
        {
            throw std::runtime_error("Failed to allocate buffer memory.");
        }

        /*
            If memory allocation was successful, then we can now associate this memory with the buffer.
            The fourth parameter is the offset within the region of memory. Since this memory is allocated specifically for this the vertex
            buffer, the offset is simply 0. If the offset is non-zero, then it is required to be divisible by memoryRequirements.alignment.
        */
        vkBindBufferMemory(device, buffer, bufferMemory, 0);
    }

    /*
        Memory transfer operations are executed using command buffers, just like drawing commands. Therefore we must first allocate a 
        temporary command buffer. We may create a separate command pool for these kinds of short-lived buffers because the implementation 
        may be able to apply memory allocation optimizations (use VK_COMMAND_POOL_CREATE_TRANSIENT_BIT flag during command pool generation).
    */
    void copyBuffer(VkBuffer srcBuffer, VkBuffer dstBuffer, VkDeviceSize size)
    {
        VkCommandBuffer commandBuffer = beginOneTimeCommands();

        /*
            Contents of buffers are transferred using the vkCmdCopyBuffer command. It takes the source and destination buffers as arguments
            and an array of regions to copy. The regions are defined in VkBufferCopy structs and consist of a source buffer offset, 
            destination buffer offset, and size. It is not possible to specify VK_WHOLE_SIZE here, unlike the vkMapMemory command.
        */
        VkBufferCopy copyRegion {};

        copyRegion.srcOffset = 0; // Optional
        copyRegion.dstOffset = 0; // Optional
        copyRegion.size = size;

        vkCmdCopyBuffer(commandBuffer, srcBuffer, dstBuffer, 1, &copyRegion);

        endOneTimeCommands(commandBuffer);
    }

    /*
        Buffers in Vulkan are regions of memory used for storing arbitrary data that can be read by the graphics card. 
        They can be used to store vertex data, but they can also be used for many other purposes.
        Buffers do not automatically allocate memory for themselves, so we must take care of memory management.
    */
    void createVertexBuffer()
    {
        VkDeviceSize bufferSize = sizeof(vertices[0]) * vertices.size();

        VkBuffer stagingBuffer;
        VkDeviceMemory stagingBufferMemory;

        // VK_BUFFER_USAGE_TRANSFER_SRC_BIT: Buffer can be used as source in a memory transfer operation.
        // VK_BUFFER_USAGE_TRANSFER_DST_BIT: Buffer can be used as destination in a memory transfer operation.
        createBuffer(
            bufferSize,
            VK_BUFFER_USAGE_TRANSFER_SRC_BIT,
            VK_MEMORY_PROPERTY_HOST_VISIBLE_BIT | VK_MEMORY_PROPERTY_HOST_COHERENT_BIT,
            stagingBuffer, 
            stagingBufferMemory
        );

        void* data;
        /*
            Memory objects created with vkAllocateMemory are not directly host (CPU) accessible.
            Memory objects created with the memory property VK_MEMORY_PROPERTY_HOST_VISIBLE_BIT are considered mappable. 
            Memory objects must be mappable in order to be successfully mapped on the host.

            Map the buffer memory into CPU accessible memory.
            This function allows us to access a region of the specified memory resource defined by an offset and size. 
            The offset and size here are 0 and bufferInfo.size, respectively. It is also possible to specify the special 
            value VK_WHOLE_SIZE to map all of the memory. The second to last parameter can be used to specify flags, 
            but there aren’t any available yet in the current API. It must be set to the value 0. 
            The last parameter is the host virtual address pointer to a region of a mappable memory object.
        */
        vkMapMemory(device, stagingBufferMemory, 0, bufferSize, 0, &data);
        // Copy the vertex data to the buffer
        memcpy(data, vertices.data(), static_cast<size_t>(bufferSize));
        /*
            Caching: Modern CPUs use caches to improve access times to frequently used data. When you write data to a memory location that is mapped to device memory (like GPU memory), the data might first be written to a cache in the CPU. Depending on the cache write policy (write-back vs. write-through), the data might not be immediately written back to the actual device memory. This caching can lead to a situation where the data appears to be written from the perspective of the CPU, but has not yet been physically transferred to the device memory.

            Memory Coherency: Memory coherency refers to the consistency of data stored in different caches or memory locations. In the context of CPU and GPU operations, it's important that both processors view consistent states of memory. However, without explicit synchronization, the CPU's view of the memory (after writing data) might differ from the GPU's view (which reads the data for rendering or computation).

            Vulkan provides explicit control over memory operations, including synchronization. When you map device memory using vkMapMemory, Vulkan offers no guarantees about the visibility of writes to this memory across different caches and memory systems.

            Delayed Copies: Because of caching mechanisms, the CPU's writes to the mapped memory might not be immediately reflected in the device memory. This delay can be due to the write-back caching policy, where writes are accumulated in the cache and flushed to the main memory at a later time.

            Visibility of Writes: Even after the data is eventually written to the device memory, there's no guarantee that the GPU will see the updated data immediately. This lack of visibility is due to the absence of memory barriers or explicit cache flushes/invalidation commands that ensure memory coherency between CPU and GPU operations.

            To address these issues, Vulkan provides mechanisms to ensure data coherency:

            Memory Barriers: Vulkan allows you to use memory barriers to synchronize access to resources and ensure that memory writes are visible across different stages of the pipeline. A memory barrier can be used after copying data to ensure that subsequent operations (like shader reads) see the updated data.

            Flushes and Invalidations: For host-visible memory types that do not guarantee coherency (VK_MEMORY_PROPERTY_HOST_COHERENT_BIT not set), applications need to explicitly flush writes to or invalidate reads from the mapped memory regions using vkFlushMappedMemoryRanges and vkInvalidateMappedMemoryRanges, respectively.
        */
        vkUnmapMemory(device, stagingBufferMemory);
        /*
            Flushing memory ranges or using a coherent memory heap means that the driver will be aware of our writes to the buffer, but it 
            doesn’t mean that they are actually visible on the GPU yet. The transfer of data to the GPU is an operation that happens in the 
            background and the specification simply tells us that it is guaranteed to be complete as of the next call to vkQueueSubmit.
        */

        /*
            The memory type that allows us to access the vertex buffer from the CPU may not be the most optimal memory type for the GPU 
            itself to read from. The most optimal memory has the VK_MEMORY_PROPERTY_DEVICE_LOCAL_BIT flag and is usually not accessible 
            by the CPU on dedicated graphics cards. We need to create two vertex buffers: One staging buffer in CPU accessible memory to 
            upload the data from the vertex array, and the final vertex buffer in GPU local memory. We then use a buffer copy command to 
            move the data from the staging buffer to the actual vertex buffer. The buffer copy command requires a queue family that 
            supports transfer operations, which is indicated using VK_QUEUE_TRANSFER_BIT. Any queue family with VK_QUEUE_GRAPHICS_BIT or 
            VK_QUEUE_COMPUTE_BIT capabilities already implicitly support VK_QUEUE_TRANSFER_BIT operations. The implementation is 
            not required to explicitly list it in queueFlags in those cases.
        */
        createBuffer(
            bufferSize,
            VK_BUFFER_USAGE_TRANSFER_DST_BIT | VK_BUFFER_USAGE_VERTEX_BUFFER_BIT,
            VK_MEMORY_PROPERTY_DEVICE_LOCAL_BIT,
            vertexBuffer,
            vertexBufferMemory
        );
        /*
            The vertexBuffer is now allocated from a memory type that is device local, which generally means that we are not able to use 
            vkMapMemory. However, we can copy data from the stagingBuffer to the vertexBuffer. We have to indicate that we intend to do that 
            by specifying the transfer source flag for the stagingBuffer and the transfer destination flag for the vertexBuffer, along with 
            the vertex buffer usage flag. Now we need to copy the contents from the staging buffer to the device local buffer:
        */
        copyBuffer(stagingBuffer, vertexBuffer, bufferSize);

        vkDestroyBuffer(device, stagingBuffer, nullptr);
        vkFreeMemory(device, stagingBufferMemory, nullptr);
    }

    void createIndexBuffer() 
    {
        VkDeviceSize bufferSize = sizeof(indices[0]) * indices.size();

        VkBuffer stagingBuffer;
        VkDeviceMemory stagingBufferMemory;

        createBuffer(
            bufferSize, 
            VK_BUFFER_USAGE_TRANSFER_SRC_BIT, 
            VK_MEMORY_PROPERTY_HOST_VISIBLE_BIT | VK_MEMORY_PROPERTY_HOST_COHERENT_BIT, 
            stagingBuffer, 
            stagingBufferMemory
        );

        void* data;
        vkMapMemory(device, stagingBufferMemory, 0, bufferSize, 0, &data);

        memcpy(data, indices.data(), static_cast<size_t>(bufferSize));

        vkUnmapMemory(device, stagingBufferMemory);

        createBuffer(
            bufferSize, 
            VK_BUFFER_USAGE_TRANSFER_DST_BIT | VK_BUFFER_USAGE_INDEX_BUFFER_BIT, 
            VK_MEMORY_PROPERTY_DEVICE_LOCAL_BIT, 
            indexBuffer, 
            indexBufferMemory
        );

        copyBuffer(stagingBuffer, indexBuffer, bufferSize);

        vkDestroyBuffer(device, stagingBuffer, nullptr);
        vkFreeMemory(device, stagingBufferMemory, nullptr);
    }

    void createImage(
        uint32_t width, 
        uint32_t height, 
        uint32_t mipLevels,
        VkFormat format, 
        VkImageTiling tiling, 
        VkImageUsageFlags usage, 
        VkMemoryPropertyFlags properties, 
        VkImage& image, 
        VkDeviceMemory& imageMemory
    ) {
        VkImageCreateInfo imageInfo {};
        /*
            The image type tells Vulkan with what kind of coordinate system the texels in the image are going to be addressed.
            It is possible to create 1D, 2D, and 3D images. One dimensional images can be used to store an array of data or gradient,
            two dimensional images are mainly used for textures, and three dimensional images can be used to store voxel volumes,
            for example. The extent field specifies the dimensions of the image, basically how many texels there are on each axis.
            That’s why depth must be 1 instead of 0.
        */
        imageInfo.sType = VK_STRUCTURE_TYPE_IMAGE_CREATE_INFO;
        imageInfo.imageType = VK_IMAGE_TYPE_2D;
        imageInfo.extent.width = width;
        imageInfo.extent.height = height;
        imageInfo.extent.depth = 1;
        /*
            When we allocate memory via vkAllocateMemory using the requirements obtained, the allocated memory will indeed cover the entire 
            set of mip levels, not just the original texture. The memory requirement calculation considers the total size needed to store 
            all mip levels because each mip level is progressively half the resolution of the previous level in each dimension (width, 
            height, and potentially depth), leading to a geometric series. The total memory required is thus slightly more than the original
            image size but less than double. Here’s how it works:

            Creating the Image: When you call vkCreateImage with mipLevels specified in VkImageCreateInfo, you're defining how many levels of detail (LODs) your image will have. Mip level 0 is the original, full-size image, and each subsequent mip level is a downsampled version, usually half the size of the previous level in each dimension.

            Querying Memory Requirements: vkGetMemoryRequirements examines the image object, which includes all mip levels, and calculates the total amount of GPU memory required to store the entire image. This calculation includes the space needed for all mip levels you've specified.

            Allocating Memory: With vkAllocateMemory, you allocate enough memory to store the entire image, including all its mip levels. This step is crucial for ensuring that when you later generate or upload mipmaps, there is sufficient space in GPU memory to store them.

            This approach allows for efficient use of mipmaps in rendering. Mipmaps are used to improve performance and visual quality by 
            selecting the appropriate level of detail for textures based on their distance from the camera, reducing aliasing and improving 
            texture sampling performance. The storage of mip levels is highly optimized by the GPU and Vulkan API to minimize the additional
            memory footprint required for these additional levels.
        */
        imageInfo.mipLevels = mipLevels;
        imageInfo.arrayLayers = 1; // our texture is not an array
        // Vulkan supports many possible image formats, but we should use the same format 
        // for the texels as the pixels in the buffer, otherwise the copy operation will fail.
        imageInfo.format = format;
        /*
            The tiling field can have one of two values:

            VK_IMAGE_TILING_LINEAR: Texels are laid out in row-major order like our pixels array.
            VK_IMAGE_TILING_OPTIMAL: Texels are laid out in an implementation defined order for optimal access.

            Unlike the layout of an image, the tiling mode cannot be changed at a later time. If we want to be able to directly access
            texels in the memory of the image, then we must use VK_IMAGE_TILING_LINEAR. We will be using a staging buffer instead of a
            staging image, so this won’t be necessary. We will be using VK_IMAGE_TILING_OPTIMAL for efficient access from the shader.
        */
        imageInfo.tiling = tiling;
        /*
            There are only two possible values for the initialLayout of an image:

            VK_IMAGE_LAYOUT_UNDEFINED: Not usable by the GPU and the very first transition will discard the texels.
            VK_IMAGE_LAYOUT_PREINITIALIZED: Not usable by the GPU, but the first transition will preserve the texels.

            There are few situations where it is necessary for the texels to be preserved during the first transition.
            One example would be if you wanted to use an image as a staging image in combination with the
            VK_IMAGE_TILING_LINEAR layout. In that case, you’d want to upload the texel data to it and then transition
            the image to be a transfer source without losing the data. In our case, however, we are first going to
            transition the image to be a transfer destination, and then copy texel data to it from a buffer object,
            so we don’t need this property and can safely use VK_IMAGE_LAYOUT_UNDEFINED.
        */
        imageInfo.initialLayout = VK_IMAGE_LAYOUT_UNDEFINED;
        /*
            The usage field has the same semantics as the one during buffer creation. The image is going to be used as destination
            for the buffer copy, so it should be set up as a transfer destination. We also want to be able to access the image from
            the shader to color our mesh, so the usage should include VK_IMAGE_USAGE_SAMPLED_BIT.
        */
        imageInfo.usage = usage;
        // The image will only be used by one queue family: the one that supports graphics, and therefore also, transfer operations.
        imageInfo.sharingMode = VK_SHARING_MODE_EXCLUSIVE;
        /*
            The samples flag is related to multisampling. This is only relevant for images that will be used as attachments,
            so we stick to one sample. There are some optional flags for images that are related to sparse images. Sparse images
            are images where only certain regions are actually backed by memory. If you were using a 3D texture for a voxel terrain,
            for example, then you could use this to avoid allocating memory to store large volumes of "air" values.
            We won’t be using it, so leave it to its default value of 0.
        */
        imageInfo.samples = VK_SAMPLE_COUNT_1_BIT;
        imageInfo.flags = 0; // Optional

        /*
            It is possible that the VK_FORMAT_R8G8B8A8_SRGB format is not supported by the graphics hardware. We should have a list of
            acceptable alternatives and go with the best one that is supported. However, support for this particular format is so
            widespread that we will skip this step. Using different formats would also require careful conversions.
        */
        if (vkCreateImage(device, &imageInfo, nullptr, &image) != VK_SUCCESS)
        {
            throw std::runtime_error("Failed to create image.");
        }

        VkMemoryRequirements memoryRequirements {};

        vkGetImageMemoryRequirements(device, image, &memoryRequirements);

        VkMemoryAllocateInfo allocInfo{};

        allocInfo.sType = VK_STRUCTURE_TYPE_MEMORY_ALLOCATE_INFO;
        allocInfo.allocationSize = memoryRequirements.size;
        allocInfo.memoryTypeIndex = findMemoryType(memoryRequirements.memoryTypeBits, properties);

        if (vkAllocateMemory(device, &allocInfo, nullptr, &imageMemory) != VK_SUCCESS)
        {
            throw std::runtime_error("Failed to allocate memory for texture image.");
        }

        vkBindImageMemory(device, image, imageMemory, 0);
    }

    void copyBufferToImage(VkBuffer buffer, VkImage image, uint32_t width, uint32_t height) 
    {
        VkCommandBuffer commandBuffer = beginOneTimeCommands();

        VkBufferImageCopy region {};
        // byte offset in the buffer at which the pixel values start
        region.bufferOffset = 0;
        // specify how the pixels are laid out in memory (i.e. we could have some padding bytes between rows of the image)
        // Specifying 0 for both indicates that the pixels are simply tightly packed.
        region.bufferRowLength = 0;
        region.bufferImageHeight = 0;
        //  indicate to which part of the image we want to copy the pixels
        region.imageSubresource.aspectMask = VK_IMAGE_ASPECT_COLOR_BIT;
        region.imageSubresource.mipLevel = 0;
        region.imageSubresource.baseArrayLayer = 0;
        region.imageSubresource.layerCount = 1;
        region.imageOffset = { 0, 0, 0 };
        region.imageExtent = 
        {
            width,
            height,
            1
        };

        /*
            The fourth parameter indicates which layout the image is currently using. We are assuming that the image 
            has already been transitioned to the layout that is optimal for copying pixels to. Right now we are only 
            copying one chunk of pixels to the whole image, but it’s possible to specify an array of VkBufferImageCopy
            to perform many different copies from this buffer to the image in one operation.
        */
        vkCmdCopyBufferToImage(
            commandBuffer,
            buffer,
            image,
            VK_IMAGE_LAYOUT_TRANSFER_DST_OPTIMAL,
            1,
            &region
        );

        endOneTimeCommands(commandBuffer);
    }

    /*
        If we were still using buffers, then we could now write a function to record and execute vkCmdCopyBufferToImage to finish the job, 
        but this command requires the image to be in the right layout first. One of the most common ways to perform layout transitions is 
        using an image memory barrier. A pipeline barrier like that is generally used to synchronize access to resources, like ensuring 
        that a write to a buffer completes before reading from it, but it can also be used to transition image layouts and transfer queue 
        family ownership when VK_SHARING_MODE_EXCLUSIVE is used. There is an equivalent buffer memory barrier to do this for buffers.

        An image memory barrier is used to:

        Synchronize access to images: It ensures that read/write operations on an image are completed before 
        the next operations start, which might depend on the previous ones.

        Transition image layouts: It changes the layout of an image, which describes how pixels are organized in memory. 
        Different stages of the pipeline require images in specific layouts for optimal access and performance. For example, 
        an image used as a color attachment for rendering might be in a layout optimized for fast color writes, but when 
        the same image needs to be read by shaders, it should be transitioned to a layout optimized for texture fetching. 
        Using an image memory barrier to perform a layout transition ensures that the transition occurs at a well-defined 
        point in the execution order, preventing subsequent operations from starting until the transition is complete.
    */
    void transitionImageLayout(VkImage image, VkFormat format, VkImageLayout oldLayout, VkImageLayout newLayout, uint32_t mipLevels)
    {
        VkCommandBuffer commandBuffer = beginOneTimeCommands();

        VkImageMemoryBarrier barrier {};

        barrier.sType = VK_STRUCTURE_TYPE_IMAGE_MEMORY_BARRIER;
        barrier.oldLayout = oldLayout;
        barrier.newLayout = newLayout;
        /*
            If you are using the barrier to transfer queue family ownership, then these two fields should be the indices of the queue 
            families. They must be set to VK_QUEUE_FAMILY_IGNORED if we don’t want to do this (which is not the default value).
        */
        barrier.srcQueueFamilyIndex = VK_QUEUE_FAMILY_IGNORED;
        barrier.dstQueueFamilyIndex = VK_QUEUE_FAMILY_IGNORED;
        // The image and subresourceRange specify the image that is affected and the specific part of the image.
        barrier.image = image;
        barrier.subresourceRange.aspectMask = VK_IMAGE_ASPECT_COLOR_BIT;
        barrier.subresourceRange.baseMipLevel = 0;
        barrier.subresourceRange.levelCount = mipLevels;
        barrier.subresourceRange.baseArrayLayer = 0;
        barrier.subresourceRange.layerCount = 1; // Our image is not an array
        /*
            Barriers are primarily used for synchronization purposes, so we must specify which types of operations that involve the resource
            must happen before the barrier, and which operations that involve the resource must wait on the barrier. We need to do that 
            despite already using vkQueueWaitIdle to manually synchronize. The right values depend on the old and new layout.
            There is a special type of image layout that supports all operations, VK_IMAGE_LAYOUT_GENERAL. The problem with it is that it 
            doesn’t necessarily offer the best performance for any operation. It is required for some special cases, like using an image 
            as both input and output, or for reading an image after it has left the preinitialized layout.
        */
        VkPipelineStageFlags sourceStage;
        VkPipelineStageFlags destinationStage;
        /*
            Transfer writes must occur in the pipeline transfer stage. Since the writes don’t have to wait on anything, we may specify an 
            empty access mask and the earliest possible pipeline stage VK_PIPELINE_STAGE_TOP_OF_PIPE_BIT for the pre-barrier operations. 
            It should be noted that VK_PIPELINE_STAGE_TRANSFER_BIT is not a real stage within the graphics and compute pipelines. It is 
            more of a pseudo-stage where transfers happen. See the documentation for more information and other examples of pseudo-stages.
        */

        // transfer writes that don’t need to wait on anything
        if (oldLayout == VK_IMAGE_LAYOUT_UNDEFINED && newLayout == VK_IMAGE_LAYOUT_TRANSFER_DST_OPTIMAL) 
        {
            barrier.srcAccessMask = 0;
            barrier.dstAccessMask = VK_ACCESS_TRANSFER_WRITE_BIT;

            sourceStage = VK_PIPELINE_STAGE_TOP_OF_PIPE_BIT;
            destinationStage = VK_PIPELINE_STAGE_TRANSFER_BIT;
        }
        // fragment shader reads should wait on transfer writes because that’s where we are going to use the texture
        else if (oldLayout == VK_IMAGE_LAYOUT_TRANSFER_DST_OPTIMAL && newLayout == VK_IMAGE_LAYOUT_SHADER_READ_ONLY_OPTIMAL) 
        {
            barrier.srcAccessMask = VK_ACCESS_TRANSFER_WRITE_BIT;
            barrier.dstAccessMask = VK_ACCESS_SHADER_READ_BIT;

            sourceStage = VK_PIPELINE_STAGE_TRANSFER_BIT;
            destinationStage = VK_PIPELINE_STAGE_FRAGMENT_SHADER_BIT;
        }
        else 
        {
            throw std::invalid_argument("Unsupported image layout transition.");
        }

        /*
            All types of pipeline barriers are submitted using the same function. The first parameter after the command buffer specifies in 
            which pipeline stage the operations occur that should happen before the barrier. The second parameter specifies the pipeline 
            stage in which operations will wait on the barrier. The pipeline stages that you are allowed to specify before and after the 
            barrier depend on how you use the resource before and after the barrier. The allowed values are listed in this table of the 
            specification. For example, if you’re going to read from a uniform after the barrier, you would specify a usage of 
            VK_ACCESS_UNIFORM_READ_BIT and the earliest shader that will read from the uniform as pipeline stage, for example 
            VK_PIPELINE_STAGE_FRAGMENT_SHADER_BIT. It would not make sense to specify a non-shader pipeline stage for this type of 
            usage and the validation layers will warn you when you specify a pipeline stage that does not match the type of usage.

            The third parameter is either 0 or VK_DEPENDENCY_BY_REGION_BIT. The latter turns the barrier into a per-region condition. I.e.
            that means that the implementation is allowed to already begin reading from the parts of a resource that were written so far.

            The last three pairs of parameters reference arrays of pipeline barriers of the three available types: memory barriers, buffer 
            memory barriers, and image memory barriers like the one we’re using here. Note that we’re not using the VkFormat parameter yet, 
            but we’ll be using that one for special transitions in the depth buffer chapter.
        */
        vkCmdPipelineBarrier(
            commandBuffer,
            sourceStage, 
            destinationStage,
            0,
            0, nullptr,
            0, nullptr,
            1, 
            &barrier
        );

        endOneTimeCommands(commandBuffer);
    }

    /*
        Loads an image and uploads it into a Vulkan image object.
        The stbi_load function takes the file path and number of channels to load as arguments. The STBI_rgb_alpha value forces the image
        to be loaded with an alpha channel, even if it doesn’t have one, which is nice for consistency with other textures in the future.
        The middle three parameters are outputs for the width, height, and actual number of channels in the image. The pointer that is
        returned is the first element in an array of pixel values. The pixels are laid out row by row with 4 bytes per pixel in the
        case of STBI_rgb_alpha for a total of texWidth * texHeight * 4 values.

        All of the helper functions that submit commands have been set up to execute synchronously by waiting for the queue to become idle. 
        For practical applications it is recommended to combine these operations in a single command buffer and execute them asynchronously 
        for higher throughput, especially the transitions and copy in the createTextureImage function. Try to experiment with this by 
        creating a setupCommandBuffer that the helper functions record commands into, and add a flushSetupCommands to execute the commands 
        that have been recorded so far.
    */
    void createTextureImage()
    {
        int textureWidth, textureHeight, textureChannels;

        stbi_uc* pixels = stbi_load(TEXTURE_PATH.c_str(), &textureWidth, &textureHeight, &textureChannels, STBI_rgb_alpha);

        VkDeviceSize imageSize = static_cast<uint64_t>(textureWidth) * static_cast<uint64_t>(textureHeight) * 4; // 4 bytes per pixel

        if (!pixels)
        {
            throw std::runtime_error("Failed to load texture image.");
        }

        /*
            Calculates the number of levels in the mip chain. 
            The max function selects the largest dimension. 
            The log2 function calculates how many times that dimension can be divided by 2 (number of mip levels).
            The floor function handles cases where the largest dimension is not a power of 2 (rounds down).
        */
        mipLevels = static_cast<uint32_t>(
            std::floor(
                std::log2(
                    std::max(textureWidth, textureHeight)
                )
            )
        ) + 1; // 1 is added so that the original image has a mip level

        VkBuffer stagingBuffer;
        VkDeviceMemory stagingBufferMemory;

        createBuffer(
            imageSize,
            VK_BUFFER_USAGE_TRANSFER_SRC_BIT,
            VK_MEMORY_PROPERTY_HOST_VISIBLE_BIT | VK_MEMORY_PROPERTY_HOST_COHERENT_BIT,
            stagingBuffer,
            stagingBufferMemory
        );

        void* data;

        vkMapMemory(device, stagingBufferMemory, 0, imageSize, 0, &data);

        memcpy(data, pixels, static_cast<size_t>(imageSize));

        vkUnmapMemory(device, stagingBufferMemory); // 1 time memory transfer

        // No longer needed in host RAM since it's now been copied into GPU memory.
        stbi_image_free(pixels);

        /*
            If the [fragment] shader will be reading this memory every frame, it makes sense to move it to device local memory. However, 
            in Vulkan, it’s better to use image objects for this purpose. Image objects will make it easier and faster to retrieve colors 
            by allowing us to use 2D coordinates. Pixels within an image object are known as texels and we will use that name from this 
            point on. vkCmdBlitImage is considered a transfer operation, so we must inform Vulkan that we intend to use the texture image 
            as both the source and destination of a transfer.
        */
        createImage(
            static_cast<uint32_t>(textureWidth),
            static_cast<uint32_t>(textureHeight),
            mipLevels,
            VK_FORMAT_R8G8B8A8_SRGB,
            VK_IMAGE_TILING_OPTIMAL,
            VK_IMAGE_USAGE_TRANSFER_SRC_BIT | VK_IMAGE_USAGE_TRANSFER_DST_BIT | VK_IMAGE_USAGE_SAMPLED_BIT,
            VK_MEMORY_PROPERTY_DEVICE_LOCAL_BIT,
            textureImage,
            textureImageMemory
        );

        // Vulkan requires image memory transfers to adhere to specific image layouts
        transitionImageLayout(
            textureImage,
            VK_FORMAT_R8G8B8A8_SRGB,
            VK_IMAGE_LAYOUT_UNDEFINED,
            VK_IMAGE_LAYOUT_TRANSFER_DST_OPTIMAL,
            mipLevels
        );

        // Mapped staging buffer -> device local image
        copyBufferToImage(
            stagingBuffer, 
            textureImage, 
            static_cast<uint32_t>(textureWidth), 
            static_cast<uint32_t>(textureHeight)
        );

        /*
            To be able to start sampling from the texture image in the shader, we need one last transition to prepare it for shader access.
            The transition now takes place while generating mipmaps, after the blit command reading from it is finished.
        */
        generateMipmaps(textureImage, textureWidth, textureHeight, mipLevels);

        vkDestroyBuffer(device, stagingBuffer, nullptr);
        vkFreeMemory(device, stagingBufferMemory, nullptr);
    }

    /*
        Our texture image now has multiple mip levels, but the staging buffer can only be used to fill mip level 0. The other levels
        are still undefined. To fill these levels we need to generate the data from the single level that we have. We will use the
        vkCmdBlitImage command. This command performs copying, scaling, and filtering operations. We will call this multiple times
        to blit data to each level of our texture image. vkCmdBlitImage is considered a transfer operation, so we must inform Vulkan
        that we intend to use the texture image as both the source and destination of a transfer. Vulkan allows us to transition each
        mip level of an image independently. Each blit will only deal with two mip levels at a time, so we can transition each level
        into the optimal layout between blits commands.
    */
    void generateMipmaps(VkImage image, int32_t textureWidth, int32_t textureHeight, uint32_t mipLevels)
    {
        VkCommandBuffer commandBuffer = beginOneTimeCommands();

        VkImageMemoryBarrier barrier {};
        // We’re going to make several transitions, so we’ll reuse this VkImageMemoryBarrier. 
        // These fields will remain the same for all barriers.
        barrier.sType = VK_STRUCTURE_TYPE_IMAGE_MEMORY_BARRIER;
        barrier.image = image;
        barrier.srcQueueFamilyIndex = VK_QUEUE_FAMILY_IGNORED;
        barrier.dstQueueFamilyIndex = VK_QUEUE_FAMILY_IGNORED;
        barrier.subresourceRange.aspectMask = VK_IMAGE_ASPECT_COLOR_BIT;
        barrier.subresourceRange.baseArrayLayer = 0;
        barrier.subresourceRange.layerCount = 1;
        barrier.subresourceRange.levelCount = 1;

        int32_t mipWidth = textureWidth;
        int32_t mipHeight = textureHeight;

        for (uint32_t i = 1; i < mipLevels; i++)
        {
            barrier.subresourceRange.baseMipLevel = i - 1;
            barrier.oldLayout = VK_IMAGE_LAYOUT_TRANSFER_DST_OPTIMAL;
            barrier.newLayout = VK_IMAGE_LAYOUT_TRANSFER_SRC_OPTIMAL;
            // wait until the source (base mip level) has finished being written to before accessing the memory
            barrier.srcAccessMask = VK_ACCESS_TRANSFER_WRITE_BIT;
            // locks memory to read safely once the previous image is ready (done writing)
            barrier.dstAccessMask = VK_ACCESS_TRANSFER_READ_BIT;

            /*
                First, we transition level i - 1 to VK_IMAGE_LAYOUT_TRANSFER_SRC_OPTIMAL. 
                This transition will wait for level i - 1 to be filled, either from the previous blit command, 
                or from vkCmdCopyBufferToImage. The current blit command will wait on this transition.
            */
            vkCmdPipelineBarrier(
                commandBuffer,
                VK_PIPELINE_STAGE_TRANSFER_BIT, // source stage
                VK_PIPELINE_STAGE_TRANSFER_BIT, // destination stage
                0,
                0, nullptr,
                0, nullptr,
                1, &barrier
            );

            /*
                srcOffsets[0]: Specifies the starting coordinates (x, y, z) of the source region. For mipmaps, this is often (0, 0, 0), 
                indicating the start of the image or the top-left corner of the source mipmap level.

                srcOffsets[1]: Defines the ending coordinates (x, y, z) of the source region. mipWidth and mipHeight specify the dimensions of 
                the source mipmap level being blitted from. The z value is typically set to 1 for a 2D image, indicating a single-layer depth.

                Together, srcOffsets[0] and srcOffsets[1] encapsulate the entire source mipmap level.

                dstOffsets[0]: Specifies the starting coordinates of the destination region, similar to srcOffsets[0], 
                typically (0, 0, 0) for mipmapping.

                dstOffsets[1]: Contains the ending coordinates of the destination region. For generating mipmaps, these are half the size 
                of the source dimensions (mipWidth / 2, mipHeight / 2), ensuring that each subsequent mipmap level is downscaled by a 
                factor of 2. The z component is again 1 for 2D images.

                If either mipWidth or mipHeight is already 1, it remains 1 to avoid a zero dimension, 
                as textures cannot have a dimension of zero size.
            */
            VkImageBlit blit {};
            // 3D region that data will be blitted from (corners in a box-like fashion, start offset to end offset)
            blit.srcOffsets[0] = { 0, 0, 0 };
            blit.srcOffsets[1] = { mipWidth, mipHeight, 1 };
            blit.srcSubresource.aspectMask = VK_IMAGE_ASPECT_COLOR_BIT;
            blit.srcSubresource.mipLevel = i - 1; // source mip level
            blit.srcSubresource.baseArrayLayer = 0;
            blit.srcSubresource.layerCount = 1;
            /*
                3D region that data will be blitted to.
                The X and Y dimensions of the dstOffsets[1] are divided by two since each mip level is half the size of the previous level.
                The Z dimension of srcOffsets[1] and dstOffsets[1] must be 1, since a 2D image has a depth of 1.
            */
            blit.dstOffsets[0] = { 0, 0, 0 };
            blit.dstOffsets[1] = { mipWidth > 1 ? mipWidth / 2 : 1, mipHeight > 1 ? mipHeight / 2 : 1, 1 };
            blit.dstSubresource.aspectMask = VK_IMAGE_ASPECT_COLOR_BIT;
            blit.dstSubresource.mipLevel = i; // destination mip level
            blit.dstSubresource.baseArrayLayer = 0;
            blit.dstSubresource.layerCount = 1;

            /*
                Note that textureImage is used for both the srcImage and dstImage parameter. This is because we are blitting between 
                different levels of the same image. The source mip level was just transitioned to VK_IMAGE_LAYOUT_TRANSFER_SRC_OPTIMAL 
                and the destination level is still in VK_IMAGE_LAYOUT_TRANSFER_DST_OPTIMAL from createTextureImage.
                Beware if you are using a dedicated transfer queue: vkCmdBlitImage must be submitted to a queue with graphics capability.
                This is because image blitting involves more than just memory transfer; it can also involve image filtering 
                (such as scaling up or down), format conversions, and potentially other operations that go beyond simple data copying.
            */
            vkCmdBlitImage(
                commandBuffer,
                image, VK_IMAGE_LAYOUT_TRANSFER_SRC_OPTIMAL,
                image, VK_IMAGE_LAYOUT_TRANSFER_DST_OPTIMAL,
                1, &blit,
                VK_FILTER_LINEAR // We use the VK_FILTER_LINEAR to enable interpolation
            );

            // We are done with the base mip level, so we transition it shader read optimal.
            barrier.oldLayout = VK_IMAGE_LAYOUT_TRANSFER_SRC_OPTIMAL;
            barrier.newLayout = VK_IMAGE_LAYOUT_SHADER_READ_ONLY_OPTIMAL;
            // wait until the source (base mip level) has finished being read before accessing the memory
            barrier.srcAccessMask = VK_ACCESS_TRANSFER_READ_BIT;
            // locks memory to read safely once the mip level is ready (done being read)
            barrier.dstAccessMask = VK_ACCESS_SHADER_READ_BIT;

            /*
                This barrier transitions mip level i - 1 to VK_IMAGE_LAYOUT_SHADER_READ_ONLY_OPTIMAL. 
                This transition waits on the current blit command to finish. 
                All sampling operations in the fragment shader will wait on this transition to finish.
            */
            vkCmdPipelineBarrier(
                commandBuffer,
                VK_PIPELINE_STAGE_TRANSFER_BIT, // source stage
                VK_PIPELINE_STAGE_FRAGMENT_SHADER_BIT, // destination stage
                0,
                0, nullptr,
                0, nullptr,
                1, &barrier
            );

            /*
                At the end of the loop, we divide the current mip dimensions by two. We check each dimension before the division to ensure 
                that dimension never becomes 0. This handles cases where the image is not square, since one of the mip dimensions would 
                reach 1 before the other dimension. When this happens, that dimension should remain 1 for all remaining levels.
            */
            if (mipWidth > 1) mipWidth /= 2;
            if (mipHeight > 1) mipHeight /= 2;
        }

        /*
            Before we end the command buffer, we insert one more pipeline barrier. This barrier transitions the last mip level from 
            VK_IMAGE_LAYOUT_TRANSFER_DST_OPTIMAL to VK_IMAGE_LAYOUT_SHADER_READ_ONLY_OPTIMAL. 
            This wasn’t handled by the loop, since the last mip level is never blitted from.
        */
        barrier.subresourceRange.baseMipLevel = mipLevels - 1;
        barrier.oldLayout = VK_IMAGE_LAYOUT_TRANSFER_DST_OPTIMAL;
        barrier.newLayout = VK_IMAGE_LAYOUT_SHADER_READ_ONLY_OPTIMAL;
        barrier.srcAccessMask = VK_ACCESS_TRANSFER_WRITE_BIT;
        barrier.dstAccessMask = VK_ACCESS_SHADER_READ_BIT;

        vkCmdPipelineBarrier(
            commandBuffer,
            VK_PIPELINE_STAGE_TRANSFER_BIT, 
            VK_PIPELINE_STAGE_FRAGMENT_SHADER_BIT, 
            0,
            0, nullptr,
            0, nullptr,
            1, &barrier
        );

        endOneTimeCommands(commandBuffer);
    }

    void createTextureImageView()
    {
        textureImageView = createImageView(textureImage, VK_FORMAT_R8G8B8A8_SRGB, VK_IMAGE_ASPECT_COLOR_BIT, mipLevels);
    }

    /*
        Textures are accessed through samplers, which will apply filtering and transformations to compute the final color that is retrieved.
        These filters are helpful to deal with problems like oversampling and undersampling. Aside from these filters, a sampler can also 
        take care of transformations. It determines what happens when you try to read texels outside the image through its addressing mode.

        Note the sampler does not reference a VkImage anywhere. The sampler is a distinct object that provides an interface 
        to extract colors from a texture. It can be applied to any image you want, whether it is 1D, 2D or 3D.
    */
    void createTextureSampler()
    {
        VkSamplerCreateInfo samplerInfo {};

        samplerInfo.sType = VK_STRUCTURE_TYPE_SAMPLER_CREATE_INFO;
        /*
            The magFilter and minFilter fields specify how to interpolate texels that are magnified or minified. 
            Magnification concerns the oversampling problem describes above, and minification concerns undersampling. 
            The choices are VK_FILTER_NEAREST and VK_FILTER_LINEAR.
        */
        samplerInfo.magFilter = VK_FILTER_LINEAR;
        samplerInfo.minFilter = VK_FILTER_LINEAR;
        /*
            The addressing mode can be specified per axis using the addressMode fields. The available values are listed below. 
            Most of these are demonstrated in the tutorial image under "Samplers". Note that the axes are called U, V and W 
            instead of X, Y and Z. This is a convention for texture space coordinates.

            VK_SAMPLER_ADDRESS_MODE_REPEAT: Repeat the texture when going beyond the image dimensions.
            VK_SAMPLER_ADDRESS_MODE_MIRRORED_REPEAT: Like repeat, but inverts the coordinates to mirror the image when going beyond the dimensions.
            VK_SAMPLER_ADDRESS_MODE_CLAMP_TO_EDGE: Take the color of the edge closest to the coordinate beyond the image dimensions.
            VK_SAMPLER_ADDRESS_MODE_MIRROR_CLAMP_TO_EDGE: Like clamp to edge, but instead uses the edge opposite to the closest edge.
            VK_SAMPLER_ADDRESS_MODE_CLAMP_TO_BORDER: Return a solid color when sampling beyond the dimensions of the image.

            It doesn’t matter which addressing mode we use here because we are not going to sample outside of the image in this tutorial. 
            However, the repeat mode is probably the most common mode, because it can be used to tile textures like floors and walls.
        */
        samplerInfo.addressModeU = VK_SAMPLER_ADDRESS_MODE_REPEAT;
        samplerInfo.addressModeV = VK_SAMPLER_ADDRESS_MODE_REPEAT;
        samplerInfo.addressModeW = VK_SAMPLER_ADDRESS_MODE_REPEAT;
        /*
            The next two fields specify if anisotropic filtering should be used. There is no reason not to use this unless performance 
            is a concern. The maxAnisotropy field limits the amount of texel samples that can be used to calculate the final color. 
            A lower value results in better performance, but lower quality results. To figure out which value we can use, we need 
            to retrieve the properties of the physical device. Per the Vulkan documentation, VkPhysicalDeviceLimits has a member 
            named 'limits'. This struct in turn has a member called maxSamplerAnisotropy and this is the maximum value we can 
            specify for maxAnisotropy. If we want to go for maximum quality, we can simply use that value directly.
        */
        VkPhysicalDeviceProperties properties {};
        vkGetPhysicalDeviceProperties(physicalDevice, &properties);
        // Anisotropic filtering is an optional device feature. The createLogicalDevice function needs to request it first.
        samplerInfo.anisotropyEnable = VK_TRUE;
        samplerInfo.maxAnisotropy = properties.limits.maxSamplerAnisotropy; // maximum texture quality
        /*
            The borderColor field specifies which color is returned when sampling beyond the image with clamp to border addressing mode. 
            It is possible to return black, white or transparent in either float or int formats. You cannot specify an arbitrary color.
        */
        samplerInfo.borderColor = VK_BORDER_COLOR_INT_OPAQUE_BLACK;
        /*
            The unnormalizedCoordinates field specifies which coordinate system we want to use to address texels in an image. 
            If this field is VK_TRUE, then we can simply use coordinates within the [0, textureWidth) and [0, textureHeight) range. 
            If it is VK_FALSE, then the texels are addressed using the [0, 1) range on all axes. Real-world applications almost always 
            use normalized coordinates because then it’s possible to use textures of varying resolutions with the exact same coordinates.
        */
        samplerInfo.unnormalizedCoordinates = VK_FALSE;
        /*
            If a comparison function is enabled, then texels will first be compared to a value, and the result of that comparison is used in filtering operations. This is mainly used for percentage-closer filtering on shadow maps.
        */
        samplerInfo.compareEnable = VK_FALSE;
        samplerInfo.compareOp = VK_COMPARE_OP_ALWAYS;
        // mipmapping filtering
        samplerInfo.mipmapMode = VK_SAMPLER_MIPMAP_MODE_LINEAR;
        samplerInfo.mipLodBias = 0.0f;
        samplerInfo.minLod = 0.0f;
        samplerInfo.maxLod = 0.0f;

        if (vkCreateSampler(device, &samplerInfo, nullptr, &textureSampler) != VK_SUCCESS)
        {
            throw std::runtime_error("Failed to create texture sampler.");
        }
    }

    /*
        All of the candidate formats contain a depth component, but the latter two also contain a stencil component. 
        We need to take that into account when performing layout transitions on images with these formats.
    */
    bool hasStencilComponent(VkFormat format) 
    {
        return format == VK_FORMAT_D32_SFLOAT_S8_UINT || format == VK_FORMAT_D24_UNORM_S8_UINT;
    }

    /*
        Takes a list of candidate formats in order from most desirable to least desirable, and checks which is the first one that is 
        supported. The support of a format depends on the tiling mode and usage, so we must also include these as parameters.
    */
    VkFormat findSupportedFormat(const std::vector<VkFormat>& candidates, VkImageTiling tiling, VkFormatFeatureFlags features)
    {
        for (VkFormat format : candidates)
        {
            VkFormatProperties properties;
            /*
                The VkFormatProperties struct contains three fields:

                linearTilingFeatures: Use cases that are supported with linear tiling
                optimalTilingFeatures: Use cases that are supported with optimal tiling
                bufferFeatures: Use cases that are supported for buffers
            */
            vkGetPhysicalDeviceFormatProperties(physicalDevice, format, &properties);

            if (tiling == VK_IMAGE_TILING_LINEAR && (properties.linearTilingFeatures & features) == features)
            {
                return format;
            }
            else if (tiling == VK_IMAGE_TILING_OPTIMAL && (properties.optimalTilingFeatures & features) == features)
            {
                return format;
            }
        }

        throw std::runtime_error("Failed to find supported format.");
    }

    /*
        Optimally tiled images are primarily used for rendering operations, such as textures in shaders or as attachments in framebuffers 
        (including depth attachments). They are the preferred choice for any resources that the GPU will frequently access.
        When using a depth image in Vulkan (for example, as a depth buffer in a rendering operation), optimal tiling is generally preferred because:
        - Depth testing is a GPU-intensive operation, and using an optimal tiling format minimizes memory access times, improving overall rendering performance.
        - Since depth images are usually written to by the GPU during rendering and read from by the GPU for depth testing, the optimal tiling format's GPU-focused optimization is beneficial.
    */
    VkFormat findDepthFormat()
    {
        return findSupportedFormat(
            { VK_FORMAT_D32_SFLOAT, VK_FORMAT_D32_SFLOAT_S8_UINT, VK_FORMAT_D24_UNORM_S8_UINT }, // formats that fit depth image requirement
            VK_IMAGE_TILING_OPTIMAL, // Optimal tiling arranges the image data in memory in a way that is optimized for access by the GPU.
            VK_FORMAT_FEATURE_DEPTH_STENCIL_ATTACHMENT_BIT
        );
    }

    /*
        The depth image should have the same resolution as the color attachment, defined by the swap chain extent, 
        an image usage appropriate for a depth attachment, optimal tiling, and device local memory. 
        Unlike the texture image, we don’t necessarily need a specific format, because we won’t be directly accessing the texels 
        from the program. It just needs to have a reasonable accuracy, at least 24 bits is common in real-world applications.
        There are several formats that fit this requirement:

        VK_FORMAT_D32_SFLOAT: 32-bit float for depth
        VK_FORMAT_D32_SFLOAT_S8_UINT: 32-bit signed float for depth and 8 bit stencil component
        VK_FORMAT_D24_UNORM_S8_UINT: 24-bit float for depth and 8 bit stencil component

        The stencil component is used for stencil tests, which is an additional test that can be combined with depth testing.
    */
    void createDepthResources()
    {
        VkFormat depthFormat = findDepthFormat();

        createImage(
            swapChainExtent.width,
            swapChainExtent.height,
            1,
            depthFormat,
            VK_IMAGE_TILING_OPTIMAL,
            VK_IMAGE_USAGE_DEPTH_STENCIL_ATTACHMENT_BIT,
            VK_MEMORY_PROPERTY_DEVICE_LOCAL_BIT,
            depthImage, 
            depthImageMemory
        );

        depthImageView = createImageView(depthImage, depthFormat, VK_IMAGE_ASPECT_DEPTH_BIT, 1);

        // We don’t need to explicitly transition the layout of the image to a depth attachment because we take care of it in the renderpass.
        //transitionImageLayout(depthImage, depthFormat, 1, VK_IMAGE_LAYOUT_UNDEFINED, VK_IMAGE_LAYOUT_DEPTH_STENCIL_ATTACHMENT_OPTIMAL);
    }

    /*
        We are going to copy new data to the uniform buffer every frame, so it doesn’t really make any sense to have a staging buffer. 
        It would just add extra overhead in this case and likely degrade performance instead of improving it.

        We should have multiple buffers because multiple frames may be in flight at the same time and we don’t want to update the buffer in 
        preparation of the next frame while a previous one is still reading from it. Thus, we need to have as many uniform buffers as we 
        have frames in flight, and write to a uniform buffer that is not currently being read by the GPU.
    */
    void createUniformBuffers()
    {
        VkDeviceSize bufferSize = sizeof(UniformBufferObject);

        uniformBuffers.resize(MAX_FRAMES_IN_FLIGHT);
        uniformBuffersMemory.resize(MAX_FRAMES_IN_FLIGHT);
        uniformBuffersMapped.resize(MAX_FRAMES_IN_FLIGHT);

        for (size_t i = 0; i < MAX_FRAMES_IN_FLIGHT; i++) 
        {
            createBuffer(
                bufferSize, 
                VK_BUFFER_USAGE_UNIFORM_BUFFER_BIT, 
                VK_MEMORY_PROPERTY_HOST_VISIBLE_BIT | VK_MEMORY_PROPERTY_HOST_COHERENT_BIT, 
                uniformBuffers[i], 
                uniformBuffersMemory[i]
            );

            /*
                Get pointers to which we can write the data later on. The buffer stays mapped to this pointer for the application’s whole 
                lifetime. This technique is called "persistent mapping" and works on all Vulkan implementations. Not having to map the 
                buffer every time we need to update it increases performances, as mapping is not free.
            */
            vkMapMemory(device, uniformBuffersMemory[i], 0, bufferSize, 0, &uniformBuffersMapped[i]);
        }
    }

    /*
        We need to provide details about every descriptor binding used in the shaders for pipeline creation.
        We have a descriptor for a uniform buffer (accessed by the vertex shader),
        and a descriptor for a combined image sampler (accessed by the fragment shader).
    */
    void createDescriptorSetLayout()
    {
        VkDescriptorSetLayoutBinding uboLayoutBinding{};
        /*
            It is possible for the shader variable to represent an array of uniform buffer objects, and descriptorCount specifies the number
            of values in the array (i.e. specify a transformation for each of the bones in a skeleton for skeletal animation, etc).
        */
        uboLayoutBinding.binding = 0;
        uboLayoutBinding.descriptorCount = 1;
        uboLayoutBinding.descriptorType = VK_DESCRIPTOR_TYPE_UNIFORM_BUFFER;
        uboLayoutBinding.stageFlags = VK_SHADER_STAGE_VERTEX_BIT; // can be a combination of VkShaderStageFlagBits values
        uboLayoutBinding.pImmutableSamplers = nullptr; // only relevant for image sampling related descriptors

        VkDescriptorSetLayoutBinding samplerLayoutBinding{};
        /*
            We intend to use the combined image sampler descriptor in the fragment shader. That’s where the color of the
            fragment is going to be determined. It is possible to use texture sampling in the vertex shader, for example
            to dynamically deform a grid of vertices by a heightmap.
        */
        samplerLayoutBinding.binding = 1;
        samplerLayoutBinding.descriptorCount = 1;
        samplerLayoutBinding.descriptorType = VK_DESCRIPTOR_TYPE_COMBINED_IMAGE_SAMPLER;
        samplerLayoutBinding.stageFlags = VK_SHADER_STAGE_FRAGMENT_BIT;
        samplerLayoutBinding.pImmutableSamplers = nullptr;

        std::array<VkDescriptorSetLayoutBinding, 2> bindings = { uboLayoutBinding, samplerLayoutBinding };

        VkDescriptorSetLayoutCreateInfo layoutInfo{};

        layoutInfo.sType = VK_STRUCTURE_TYPE_DESCRIPTOR_SET_LAYOUT_CREATE_INFO;
        layoutInfo.bindingCount = static_cast<uint32_t>(bindings.size());
        layoutInfo.pBindings = bindings.data();
        // flags and pNext: not extending this structure

        if (vkCreateDescriptorSetLayout(device, &layoutInfo, nullptr, &descriptorSetLayout) != VK_SUCCESS)
        {
            throw std::runtime_error("Failed to create descriptor set layout.");
        }
    }

    /*
        Descriptor pools allow for the allocation of memory for a fixed number of descriptors and descriptor sets upfront. By specifying the
        number and type of descriptors that can be allocated from a pool, Vulkan implementations can optimize memory usage and management. 
        This approach minimizes the overhead associated with dynamic memory allocation and deallocation, which can be costly in terms of performance.
        Since Vulkan gives developers close control over hardware resources, descriptor pools require you to explicitly define how many 
        descriptors of each type will be needed. This explicit control helps avoid unnecessary memory usage and ensures that the GPU 
        allocates resources efficiently. By allocating descriptors from pools, Vulkan can batch resource management operations, which can 
        significantly improve performance. Instead of handling descriptors individually, Vulkan deals with them in batches, reducing the 
        overhead of resource allocation and freeing. Descriptor pools help reduce driver overhead by limiting the need for the driver to 
        constantly manage resource allocations and deallocations. With a predefined pool, the driver has a clear understanding of the 
        resources required, allowing for optimized handling of those resources.
    */
    void createDescriptorPool()
    {
        std::array<VkDescriptorPoolSize, 2> poolSizes {};

        poolSizes[0].type = VK_DESCRIPTOR_TYPE_UNIFORM_BUFFER;
        poolSizes[0].descriptorCount = static_cast<uint32_t>(MAX_FRAMES_IN_FLIGHT); // distinct UBO per frame
        // creating a larger descriptor pool to make room for the allocation of the combined image sampler, one per frame
        poolSizes[1].type = VK_DESCRIPTOR_TYPE_COMBINED_IMAGE_SAMPLER;
        poolSizes[1].descriptorCount = static_cast<uint32_t>(MAX_FRAMES_IN_FLIGHT);

        VkDescriptorPoolCreateInfo poolInfo {};
        /*
            The structure has an optional flag similar to command pools that determines if individual descriptor sets can be freed or not: 
            VK_DESCRIPTOR_POOL_CREATE_FREE_DESCRIPTOR_SET_BIT. We are not going to touch the descriptor set after creating it, 
            so we don’t need this flag. You can leave flags to its default value of 0.
        */
        poolInfo.sType = VK_STRUCTURE_TYPE_DESCRIPTOR_POOL_CREATE_INFO;
        poolInfo.poolSizeCount = static_cast<uint32_t>(poolSizes.size());
        poolInfo.pPoolSizes = poolSizes.data(); // could be multiple sizes
        poolInfo.maxSets = static_cast<uint32_t>(MAX_FRAMES_IN_FLIGHT); // 1 descriptor set per frame

        if (vkCreateDescriptorPool(device, &poolInfo, nullptr, &descriptorPool) != VK_SUCCESS)
        {
            throw std::runtime_error("Failed to create descriptor pool.");
        }
    }

    /*
        Descriptor sets are used to provide shaders with access to resources like buffers and images.
        Vulkan requires explicit management of these GPU resources via descriptor pools.
    */
    void createDescriptorSets()
    {
        /*
            We create one descriptor set for each frame in flight, all with the same layout. 
            We need all the copies of the layout because the next function expects an array matching the number of sets.
        */
        std::vector<VkDescriptorSetLayout> layouts(MAX_FRAMES_IN_FLIGHT, descriptorSetLayout);

        VkDescriptorSetAllocateInfo allocInfo {};

        allocInfo.sType = VK_STRUCTURE_TYPE_DESCRIPTOR_SET_ALLOCATE_INFO;
        allocInfo.descriptorSetCount = static_cast<uint32_t>(MAX_FRAMES_IN_FLIGHT);
        allocInfo.descriptorPool = descriptorPool;
        allocInfo.pSetLayouts = layouts.data();

        descriptorSets.resize(MAX_FRAMES_IN_FLIGHT);

        // The call to vkAllocateDescriptorSets will allocate descriptor sets, each with one uniform buffer descriptor.
        if (vkAllocateDescriptorSets(device, &allocInfo, descriptorSets.data()) != VK_SUCCESS)
        {
            throw std::runtime_error("Failed to allocate descriptor sets.");
        }

        // The descriptor sets have been allocated, and now the descriptors within need to be configured:
        for (size_t i = 0; i < MAX_FRAMES_IN_FLIGHT; i++)
        {
            // specifies the buffer and the region within it that contains the data for the descriptor
            VkDescriptorBufferInfo bufferInfo {};
            // If we are overwriting the whole buffer (which we are), then it is also possible to use the VK_WHOLE_SIZE value for the range.
            bufferInfo.buffer = uniformBuffers[i];
            bufferInfo.offset = 0;
            bufferInfo.range = sizeof(UniformBufferObject);

            // bind the actual image and sampler resources to the descriptors in the descriptor set
            VkDescriptorImageInfo imageInfo {};

            imageInfo.imageLayout = VK_IMAGE_LAYOUT_SHADER_READ_ONLY_OPTIMAL;
            imageInfo.imageView = textureImageView;
            imageInfo.sampler = textureSampler;

            // Describes how the descriptors will be updated / written to.
            std::array<VkWriteDescriptorSet, 2> descriptorWrites {};

            /*
                The first two fields specify the descriptor set to update and the binding. We gave our uniform buffer binding index 0. 
                Descriptors can be arrays, so we also need to specify the first index in the array that we want to update. 
                We are not using an array, so the index is simply 0.
            */
            descriptorWrites[0].sType = VK_STRUCTURE_TYPE_WRITE_DESCRIPTOR_SET;
            descriptorWrites[0].dstSet = descriptorSets[i];
            descriptorWrites[0].dstBinding = 0;
            descriptorWrites[0].dstArrayElement = 0;
            /*
                We need to specify the type of descriptor again. It’s possible to update multiple descriptors at once in an array, 
                starting at index dstArrayElement. The descriptorCount field specifies how many array elements we want to update.
            */
            descriptorWrites[0].descriptorType = VK_DESCRIPTOR_TYPE_UNIFORM_BUFFER;
            descriptorWrites[0].descriptorCount = 1;
            /*
                The last field (1/3) references an array with descriptorCount structs that actually configure the descriptors. It depends 
                on the type of descriptor which one of the three you actually need to use. The pBufferInfo field is used for descriptors 
                that refer to buffer data, pImageInfo is used for descriptors that refer to image data, and pTexelBufferView is used for 
                descriptors that refer to buffer views. Our descriptor is based on buffers, so we are using pBufferInfo.
            */
            descriptorWrites[0].pBufferInfo = &bufferInfo;
            descriptorWrites[0].pImageInfo = nullptr; // Optional
            descriptorWrites[0].pTexelBufferView = nullptr; // Optional

            descriptorWrites[1].sType = VK_STRUCTURE_TYPE_WRITE_DESCRIPTOR_SET;
            descriptorWrites[1].dstSet = descriptorSets[i];
            descriptorWrites[1].dstBinding = 1;
            descriptorWrites[1].dstArrayElement = 0;
            descriptorWrites[1].descriptorType = VK_DESCRIPTOR_TYPE_COMBINED_IMAGE_SAMPLER;
            descriptorWrites[1].descriptorCount = 1;
            descriptorWrites[1].pImageInfo = &imageInfo;
            descriptorWrites[1].pBufferInfo = nullptr; // Optional
            descriptorWrites[1].pTexelBufferView = nullptr; // Optional

            /*
                The updates are applied using vkUpdateDescriptorSets. It accepts two kinds of arrays as parameters: 
                an array of VkWriteDescriptorSet and an array of VkCopyDescriptorSet. 
                The latter can be used to copy descriptors to each other, as its name implies.
            */
            vkUpdateDescriptorSets(device, static_cast<uint32_t>(descriptorWrites.size()), descriptorWrites.data(), 0, nullptr);
        }
    }

    /*
        Vulkan expects shader code to be passed as a pointer to uint32_t in the pCode field of the VkShaderModuleCreateInfo struct, 
        aligned to a 4-byte boundary, because shaders are consumed as an array of 32-bit words. This is because GPU hardware and the 
        SPIR-V shader bytecode format are designed to work with 32-bit instructions. In practice, this means that when you store shader 
        code in a std::vector<char>, even though char could technically be stored at any byte boundary, the memory allocated by the 
        vector's default allocator will be aligned in such a way that converting the address to a uint32_t* pointer for Vulkan's use 
        will still respect the 4-byte alignment requirement. This alignment ensures that when Vulkan accesses the shader code as an 
        array of 32-bit words, those accesses are correctly aligned according to the hardware and API requirements, thus maintaining 
        performance and preventing potential errors. Using reinterpret_cast to convert char* to uint32_t* makes the programmer's 
        intent explicit and maintains type safety. The use of const in this context is also aligned with the Vulkan API's requirement 
        that the shader code should not be modified during shader module creation, although the const qualifier may need to be adjusted 
        depending on the API's expectations and the const correctness of the data.
    */
    VkShaderModule createShaderModule(const std::vector<char>& code)
    {
        VkShaderModuleCreateInfo createInfo {};

        createInfo.sType = VK_STRUCTURE_TYPE_SHADER_MODULE_CREATE_INFO;
        createInfo.codeSize = code.size();
        /*
            C-style casts like (uint32_t*)code.data() are versatile but potentially unsafe, as they do not provide 
            compile-time type checking. They can perform a combination of static_cast, reinterpret_cast, const_cast, 
            and even dynamic_cast operations, depending on the context.

            reinterpret_cast<const uint32_t*>(code.data()) explicitly reinterprets the pointer type without changing 
            the bit pattern of the pointer value. reinterpret_cast is more specific and safer than C-style casts because 
            it limits the kinds of conversions that can be performed, making the programmer's intent clearer and reducing 
            the risk of unintended conversions. Adding const in this cast also indicates that the pointed-to data should 
            not be modified, which is important for type safety and maintaining const correctness.
        */
        createInfo.pCode = reinterpret_cast<const uint32_t*>(code.data());

        VkShaderModule shaderModule {};

        if (vkCreateShaderModule(device, &createInfo, nullptr, &shaderModule) != VK_SUCCESS)
        {
            throw std::runtime_error("Failed to create shader module.");
        }

        // The buffer with the code can be freed immediately after creating the shader module.
        //delete code.data();

        return shaderModule;
    }

    /*
        In Vulkan you have to be explicit about most pipeline states as it will be baked into an immutable pipeline state object.
        The compilation and linking of the SPIR-V bytecode to machine code for execution by the GPU doesn’t happen until the graphics
        pipeline is created. That means that we’re allowed to destroy the shader modules again as soon as pipeline creation is 
        finished, which is why we’ll make them local variables in the createGraphicsPipeline function instead of class members.
    */
    void createGraphicsPipeline()
    {
        auto vertexShaderCode = readFile("shaders/vert.spv");
        auto fragmentShaderCode = readFile("shaders/frag.spv");

        //std::cout << vertexShaderCode.size() << std::endl;
        //std::cout << fragmentShaderCode.size() << std::endl;

        auto vertexModule = createShaderModule(vertexShaderCode);
        auto fragmentModule = createShaderModule(fragmentShaderCode);

        VkPipelineShaderStageCreateInfo vertexStageInfo {};

        vertexStageInfo.sType = VK_STRUCTURE_TYPE_PIPELINE_SHADER_STAGE_CREATE_INFO;
        vertexStageInfo.stage = VK_SHADER_STAGE_VERTEX_BIT; // there is an enum value for each of the programmable stages
        vertexStageInfo.module = vertexModule;
        /*
            Specifies the function to invoke (entrypoint). Tt’s possible to combine multiple fragment shaders into 
            a single shader module and use different entry points to differentiate between their behaviors.
        */
        vertexStageInfo.pName = "main";
        /*
            There is one more (optional) member, pSpecializationInfo, that allows you to specify values for shader constants. 
            You can use a single shader module where its behavior can be configured at pipeline creation by specifying different 
            values for the constants used in it. This is more efficient than configuring the shader using variables at render time, 
            because the compiler can do optimizations like eliminating if statements that depend on these values. If you don’t have 
            any constants like that, then you can set the member to nullptr, which our struct initialization does automatically.
        */

        VkPipelineShaderStageCreateInfo fragmentStageInfo {};

        fragmentStageInfo.sType = VK_STRUCTURE_TYPE_PIPELINE_SHADER_STAGE_CREATE_INFO;
        fragmentStageInfo.stage = VK_SHADER_STAGE_FRAGMENT_BIT;
        fragmentStageInfo.module = fragmentModule;
        fragmentStageInfo.pName = "main";

        VkPipelineShaderStageCreateInfo shaderStages[] = { vertexStageInfo, fragmentStageInfo };

        // Fixed functions start

        auto bindingDescription = Vertex::getBindingDescription();
        auto attributeDescriptions = Vertex::getAttributeDescriptions();
        /*
            Describes the format of the vertex data that will be passed to the vertex shader in roughly two ways:

            Bindings: spacing between data and whether the data is per-vertex or per-instance (see instancing)
            Attribute descriptions: type of the attributes passed to the vertex shader, which binding to load them from and at which offset
        */
        VkPipelineVertexInputStateCreateInfo vertexInputInfo {};
        
        vertexInputInfo.sType = VK_STRUCTURE_TYPE_PIPELINE_VERTEX_INPUT_STATE_CREATE_INFO;
        vertexInputInfo.vertexBindingDescriptionCount = 1;
        vertexInputInfo.pVertexBindingDescriptions = &bindingDescription; // could be an array with multiple bindings
        vertexInputInfo.vertexAttributeDescriptionCount = static_cast<uint32_t>(attributeDescriptions.size());
        vertexInputInfo.pVertexAttributeDescriptions = attributeDescriptions.data();

        VkPipelineInputAssemblyStateCreateInfo inputAssembly {};
        /*
            The topology member describes what kind of geometry will be drawn from the vertices and can have values like:

            VK_PRIMITIVE_TOPOLOGY_POINT_LIST: points from vertices

            VK_PRIMITIVE_TOPOLOGY_LINE_LIST: line from every 2 vertices without reuse

            VK_PRIMITIVE_TOPOLOGY_LINE_STRIP: the end vertex of every line is used as start vertex for the next line

            VK_PRIMITIVE_TOPOLOGY_TRIANGLE_LIST: triangle from every 3 vertices without reuse

            `VK_PRIMITIVE_TOPOLOGY_TRIANGLE_STRIP `: the second and third vertex of every triangle are used as first two vertices of the next triangle

            Normally, the vertices are loaded from the vertex buffer by index in sequential order, but with an element buffer 
            you can specify the indices to use yourself. This allows you to perform optimizations like reusing vertices. If 
            you set the primitiveRestartEnable member to VK_TRUE, then it’s possible to break up lines and triangles in the 
            _STRIP topology modes by using a special index of 0xFFFF or 0xFFFFFFFF.
        */
        inputAssembly.sType = VK_STRUCTURE_TYPE_PIPELINE_INPUT_ASSEMBLY_STATE_CREATE_INFO;
        inputAssembly.topology = VK_PRIMITIVE_TOPOLOGY_TRIANGLE_LIST; // We only draw triangles in this project
        inputAssembly.primitiveRestartEnable = VK_FALSE;

        /*
            A viewport describes the region of the framebuffer that the output will be rendered to. This will almost always be 
            (0, 0) to (width, height). The swap chain images will be used as framebuffers later, so we should stick to their size.
            The minDepth and maxDepth values specify the range of depth values for the framebuffer and must be within [0.0f, 1.0f].
        */
        /*
            Viewport(s) and scissor rectangle(s) can either be specified as a static part of the pipeline or as a dynamic
            state set in the command buffer. While the former is more in line with the other states it’s often convenient
            to make viewport and scissor state dynamic as it gives you a lot more flexibility. This is very common and all
            implementations can handle this dynamic state without a performance penalty. When opting for dynamic viewport(s)
            and scissor rectangle(s) you need to enable the respective dynamic states for the pipeline.
            The actual viewport(s) and scissor rectangle(s) would then later be set up at drawing time.
        */
        std::vector<VkDynamicState> dynamicStates =
        {
            VK_DYNAMIC_STATE_VIEWPORT,
            VK_DYNAMIC_STATE_SCISSOR
        };

        VkPipelineDynamicStateCreateInfo dynamicState {};

        dynamicState.sType = VK_STRUCTURE_TYPE_PIPELINE_DYNAMIC_STATE_CREATE_INFO;
        dynamicState.dynamicStateCount = static_cast<uint32_t>(dynamicStates.size());
        dynamicState.pDynamicStates = dynamicStates.data();

        VkPipelineViewportStateCreateInfo viewportState {};
        /*
            Without dynamic state, the viewport and scissor rectangle need to be set in the pipeline using the 
            VkPipelineViewportStateCreateInfo struct. This makes the viewport and scissor rectangle for this pipeline 
            immutable. Any changes required to these values would require a new pipeline to be created with the new values.
        */
        viewportState.sType = VK_STRUCTURE_TYPE_PIPELINE_VIEWPORT_STATE_CREATE_INFO;
        viewportState.viewportCount = 1;
        //viewportState.pViewports = &viewport;
        viewportState.scissorCount = 1;
        //viewportState.pScissors = &scissor;

        /*
            The rasterizer takes the geometry that is shaped by the vertices from the vertex shader and turns it into fragments 
            to be colored by the fragment shader. It also performs depth testing, face culling and the scissor test, and it can 
            be configured to output fragments that fill entire polygons or just the edges (wireframe rendering).
        */
        VkPipelineRasterizationStateCreateInfo rasterizer {};

        rasterizer.sType = VK_STRUCTURE_TYPE_PIPELINE_RASTERIZATION_STATE_CREATE_INFO;
        /*
            If depthClampEnable is set to VK_TRUE, then fragments that are beyond the near and far planes are clamped to them as 
            opposed to discarding them. This is useful in some special cases like shadow maps. Using this requires enabling a GPU feature 
            because not all graphics cards support this. For shadow maps, rendering objects that are partially or fully outside the 
            camera's view can be necessary to correctly calculate shadows for objects that are within view. Depth clamping allows these 
            out-of-view objects to still contribute to the shadow map, improving the quality of shadows at the edges of the view frustum.

            Normally, fragments that would end up outside the near or far planes after the perspective division (which converts clip space 
            coordinates to normalized device coordinates) are discarded. When depthClampEnable is set to VK_TRUE, these fragments are not 
            discarded. Instead, their depth values are clamped to the near or far plane values. The key point is that while the clipping 
            operations to the side planes of the frustum still occur, the depth clamping modifies how out-of-bound depth values are treated,
            preventing their outright discarding based solely on depth.
        */
        rasterizer.depthClampEnable = VK_FALSE;
        // If VK_TRUE, then geometry never passes through the rasterizer stage. This basically disables any output to the framebuffer.
        rasterizer.rasterizerDiscardEnable = VK_FALSE;
        /*
            The polygonMode determines how fragments are generated for geometry. The following modes are available:

            VK_POLYGON_MODE_FILL: fill the area of the polygon with fragments

            VK_POLYGON_MODE_LINE: polygon edges are drawn as lines

            VK_POLYGON_MODE_POINT: polygon vertices are drawn as points

            Using any mode other than fill requires enabling a GPU feature because not all graphics cards support the other modes.
        */
        rasterizer.polygonMode = VK_POLYGON_MODE_FILL;
        /*
            The lineWidth member describes the thickness of lines in terms of number of fragments. The maximum line width that is 
            supported depends on the hardware and any line thicker than 1.0f requires you to enable the wideLines GPU feature.
        */
        rasterizer.lineWidth = 1.0f;
        /*
            The cullMode variable determines the type of face culling to use. You can disable culling, 
            cull the front faces, cull the back faces, or both. The frontFace variable specifies the 
            vertex order for faces to be considered front-facing and can be clockwise or counterclockwise.
            Because of the Y-flip we did in the projection matrix, the vertices are now being drawn in counter-clockwise order 
            instead of clockwise order. This causes backface culling to kick in and prevents any geometry from being drawn, 
            unless we make the front face match this (VK_FRONT_FACE_COUNTER_CLOCKWISE).
        */
        rasterizer.cullMode = VK_CULL_MODE_BACK_BIT;
        rasterizer.frontFace = VK_FRONT_FACE_COUNTER_CLOCKWISE;
        /*
            The rasterizer can alter the depth values by adding a constant value or biasing them based on a fragment’s slope. 
            This is sometimes used for shadow mapping, but we won’t be using it.
        */
        rasterizer.depthBiasEnable = VK_FALSE;
        rasterizer.depthBiasConstantFactor = 0.0f; // Optional
        rasterizer.depthBiasClamp = 0.0f; // Optional
        rasterizer.depthBiasSlopeFactor = 0.0f; // Optional

        /*
            Configures multisampling, which is one of the ways to perform anti-aliasing. 
            It works by combining the fragment shader results of multiple polygons that rasterize to the same pixel. 
            This mainly occurs along edges, which is also where the most noticeable aliasing artifacts occur. 
            Because it doesn’t need to run the fragment shader multiple times if only one polygon maps to a pixel, 
            it is significantly less expensive than simply rendering to a higher resolution and then downscaling. 
            Enabling it requires enabling a GPU feature because not all graphics cards support it.
        */
        VkPipelineMultisampleStateCreateInfo multisampling {}; // disabled for now

        multisampling.sType = VK_STRUCTURE_TYPE_PIPELINE_MULTISAMPLE_STATE_CREATE_INFO;
        multisampling.sampleShadingEnable = VK_FALSE;
        multisampling.rasterizationSamples = VK_SAMPLE_COUNT_1_BIT;
        multisampling.minSampleShading = 1.0f; // Optional
        multisampling.pSampleMask = nullptr; // Optional
        multisampling.alphaToCoverageEnable = VK_FALSE; // Optional
        multisampling.alphaToOneEnable = VK_FALSE; // Optional

        // A depth stencil state must always be specified if the render pass contains a depth stencil attachment.
        VkPipelineDepthStencilStateCreateInfo depthStencil {};
        /*
            The depthTestEnable field specifies if the depth of new fragments should be compared to the depth buffer to see if they should
            be discarded. The depthWriteEnable field specifies if the new depth of fragments that pass the depth test should actually be
            written to the depth buffer.
        */
        depthStencil.sType = VK_STRUCTURE_TYPE_PIPELINE_DEPTH_STENCIL_STATE_CREATE_INFO;
        depthStencil.depthTestEnable = VK_TRUE;
        depthStencil.depthWriteEnable = VK_TRUE;
        // Specifies the comparison that is performed to keep or discard fragments. 
        // The convention is: lower depth = closer, so the depth of new fragments should be less.
        depthStencil.depthCompareOp = VK_COMPARE_OP_LESS;
        /*
            The depthBoundsTestEnable, minDepthBounds, and maxDepthBounds fields are used for the optional depth bound test. 
            This allows us to only keep fragments that fall within the specified depth range. We won’t be using this functionality.
        */
        depthStencil.depthBoundsTestEnable = VK_FALSE;
        depthStencil.minDepthBounds = 0.0f; // Optional
        depthStencil.maxDepthBounds = 1.0f; // Optional
        /*
            The last three fields configure stencil buffer operations, which we also won’t be using in this tutorial. If you want to use 
            these operations, then you will have to make sure that the format of the depth/stencil image contains a stencil component.
        */
        depthStencil.stencilTestEnable = VK_FALSE;
        depthStencil.front = {}; // Optional
        depthStencil.back = {}; // Optional

        /*
            After a fragment shader has returned a color, it needs to be combined with the color that is already in the framebuffer. 
            This transformation is known as color blending and there are two ways to do it:

            - Mix the old and new value to produce a final color (per-framebuffer struct below)
            - Combine the old and new value using a bitwise operation

            There are two types of structs to configure color blending. The first struct, VkPipelineColorBlendAttachmentState 
            contains the configuration per attached framebuffer and the second struct, VkPipelineColorBlendStateCreateInfo 
            contains the global color blending settings. In our case we only have one framebuffer.
        */
        VkPipelineColorBlendAttachmentState colorBlendAttachment {};
        /*
            If blendEnable is set to VK_FALSE, then the new color from the fragment shader is passed through unmodified. 
            Otherwise, the two mixing operations are performed to compute a new color. 
            The resulting color is AND’d with the colorWriteMask to determine which channels are actually passed through.

            The most common way to use color blending is to implement alpha blending, where we want 
            the new color to be blended with the old color based on its opacity.
        */
        colorBlendAttachment.colorWriteMask = VK_COLOR_COMPONENT_R_BIT | VK_COLOR_COMPONENT_G_BIT | VK_COLOR_COMPONENT_B_BIT | VK_COLOR_COMPONENT_A_BIT;
        colorBlendAttachment.blendEnable = VK_TRUE; // VK_FALSE and below comments means no blending (ignore destination pixels / overwrite)
        colorBlendAttachment.srcColorBlendFactor = VK_BLEND_FACTOR_SRC_ALPHA; // VK_BLEND_FACTOR_ONE
        colorBlendAttachment.dstColorBlendFactor = VK_BLEND_FACTOR_ONE_MINUS_SRC_ALPHA; // VK_BLEND_FACTOR_ZERO
        colorBlendAttachment.colorBlendOp = VK_BLEND_OP_ADD;
        colorBlendAttachment.srcAlphaBlendFactor = VK_BLEND_FACTOR_ONE;
        colorBlendAttachment.dstAlphaBlendFactor = VK_BLEND_FACTOR_ZERO;
        colorBlendAttachment.alphaBlendOp = VK_BLEND_OP_ADD;

        /*
            References the array of structures for all of the framebuffers and allows us to 
            set blend constants that we can use as blend factors in the above calculations.
        */
        VkPipelineColorBlendStateCreateInfo colorBlending {};
        /*
            If you want to use the second method of blending (bitwise combination), then you should set logicOpEnable to VK_TRUE. 
            The bitwise operation can then be specified in the logicOp field. Note that this will automatically disable the first 
            method, as if you had set blendEnable to VK_FALSE for every attached framebuffer! The colorWriteMask will also be used 
            in this mode to determine which channels in the framebuffer will actually be affected. It is also possible to disable 
            both modes, as we’ve done here, in which case the fragment colors will be written to the framebuffer unmodified.
        */
        colorBlending.sType = VK_STRUCTURE_TYPE_PIPELINE_COLOR_BLEND_STATE_CREATE_INFO;
        colorBlending.logicOpEnable = VK_FALSE;
        colorBlending.logicOp = VK_LOGIC_OP_COPY; // Optional
        colorBlending.attachmentCount = 1;
        colorBlending.pAttachments = &colorBlendAttachment;
        colorBlending.blendConstants[0] = 0.0f; // Optional
        colorBlending.blendConstants[1] = 0.0f; // Optional
        colorBlending.blendConstants[2] = 0.0f; // Optional
        colorBlending.blendConstants[3] = 0.0f; // Optional

        // Fixed functions end

        /*
            Pipeline layout: the uniform and push values referenced by the shader that can be updated at draw time.

            We can use uniform values in shaders, which are globals similar to dynamic state variables that can be changed at 
            drawing time to alter the behavior of our shaders without having to recreate them. They are commonly used to pass 
            the transformation matrix to the vertex shader, or to create texture samplers in the fragment shader.

            These uniform values need to be specified during pipeline creation by creating a VkPipelineLayout object. 
            The structure also specifies push constants, which are another way of passing dynamic values to shaders.
            We need to specify the descriptor set layout to tell Vulkan which descriptors the shaders will be using.
            It’s possible to specify multiple descriptor set layouts here, even though a single one already includes 
            all of the bindings, because of descriptor pools and descriptor sets.
        */
        VkPipelineLayoutCreateInfo pipelineLayoutInfo {};
        // required even if the pipeline does not use any
        pipelineLayoutInfo.sType = VK_STRUCTURE_TYPE_PIPELINE_LAYOUT_CREATE_INFO;
        pipelineLayoutInfo.setLayoutCount = 1;
        pipelineLayoutInfo.pSetLayouts = &descriptorSetLayout;
        pipelineLayoutInfo.pushConstantRangeCount = 0; // Optional
        pipelineLayoutInfo.pPushConstantRanges = nullptr; // Optional

        if (vkCreatePipelineLayout(device, &pipelineLayoutInfo, nullptr, &pipelineLayout) != VK_SUCCESS) 
        {
            throw std::runtime_error("Failed to create pipeline layout.");
        }

        VkGraphicsPipelineCreateInfo pipelineInfo {};

        // Referencing the shader tages
        pipelineInfo.sType = VK_STRUCTURE_TYPE_GRAPHICS_PIPELINE_CREATE_INFO;
        pipelineInfo.stageCount = 2; // 1 vertex and 1 fragment
        pipelineInfo.pStages = shaderStages;
        // Referencing all of the structures describing the fixed-function stage.
        pipelineInfo.pVertexInputState = &vertexInputInfo;
        pipelineInfo.pInputAssemblyState = &inputAssembly;
        pipelineInfo.pViewportState = &viewportState;
        pipelineInfo.pRasterizationState = &rasterizer;
        pipelineInfo.pMultisampleState = &multisampling;
        pipelineInfo.pDepthStencilState = &depthStencil;
        pipelineInfo.pColorBlendState = &colorBlending;
        pipelineInfo.pDynamicState = &dynamicState;
        // the pipeline layout is a Vulkan handle rather than a struct pointer
        pipelineInfo.layout = pipelineLayout;
        /*
            Referencing the render pass and the index of the subpass where this graphics pipeline will be used. 
            It is also possible to use other render passes with this pipeline instead of this specific instance, 
            but they have to be compatible with renderPass. The requirements for compatibility are described in the documentation.
        */
        pipelineInfo.renderPass = renderPass;
        pipelineInfo.subpass = 0;
        /*
            Vulkan allows us to create a new graphics pipeline by deriving from an existing pipeline. The idea of pipeline derivatives 
            is that it is less expensive to set up pipelines when they have much functionality in common with an existing pipeline and 
            switching between pipelines from the same parent can also be done quicker. You can either specify the handle of an existing 
            pipeline with basePipelineHandle or reference another pipeline that is about to be created by index with basePipelineIndex. 
            Right now there is only a single pipeline, so we simply specify a null handle and an invalid index. These values are only 
            used if the VK_PIPELINE_CREATE_DERIVATIVE_BIT flag is also specified in the flags field of VkGraphicsPipelineCreateInfo.
        */
        pipelineInfo.basePipelineHandle = VK_NULL_HANDLE; // Optional
        pipelineInfo.basePipelineIndex = -1; // Optional

        /*
            The vkCreateGraphicsPipelines function is designed to take multiple VkGraphicsPipelineCreateInfo objects and create multiple 
            VkPipeline objects in a single call. The second parameter, for which we’ve passed the VK_NULL_HANDLE argument, references an 
            optional VkPipelineCache object. A pipeline cache can be used to store and reuse data relevant to pipeline creation across 
            multiple calls to vkCreateGraphicsPipelines and even across program executions if the cache is stored to a file. This makes 
            it possible to significantly speed up pipeline creation at a later time. We’ll get into this in the pipeline cache chapter.
        */
        if (vkCreateGraphicsPipelines(device, VK_NULL_HANDLE, 1, &pipelineInfo, nullptr, &graphicsPipeline) != VK_SUCCESS)
        {
            throw std::runtime_error("Failed to create graphics pipeline.");
        }

        vkDestroyShaderModule(device, fragmentModule, nullptr);
        vkDestroyShaderModule(device, vertexModule, nullptr);
    }

    /*
        A framebuffer object references all of the VkImageView objects that represent the attachments. 
        In our case that will be only a single one: the color attachment. However, the image that we have to use for the attachment 
        depends on which image the swap chain returns when we retrieve one for presentation. That means that we have to create a 
        framebuffer for all of the images in the swap chain and use the one that corresponds to the retrieved image at drawing time.
        Render pass = the how and Framebuffer = the what
    */
    void createFramebuffers()
    {
        swapChainFramebuffers.resize(swapChainImageViews.size());

        for (size_t i = 0; i < swapChainImageViews.size(); i++)
        {
            /*
                The color attachment differs for every swap chain image, but the same depth image can be used by all of them 
                because only a single subpass is running at the same time due to our semaphores.
            */
            std::array<VkImageView, 2> attachments =
            {
                swapChainImageViews[i],
                depthImageView
            };

            VkFramebufferCreateInfo framebufferInfo {};

            framebufferInfo.sType = VK_STRUCTURE_TYPE_FRAMEBUFFER_CREATE_INFO;
            // You can only use a framebuffer with the render passes that it is compatible with, 
            // which roughly means that they use the same number and type of attachments.
            framebufferInfo.renderPass = renderPass;
            framebufferInfo.attachmentCount = static_cast<uint32_t>(attachments.size());
            framebufferInfo.pAttachments = attachments.data();
            framebufferInfo.width = swapChainExtent.width;
            framebufferInfo.height = swapChainExtent.height;
            framebufferInfo.layers = 1; // our swap chain images are single images

            if (vkCreateFramebuffer(device, &framebufferInfo, nullptr, &swapChainFramebuffers[i]) != VK_SUCCESS)
            {
                throw std::runtime_error("Failed to create framebuffer for swap chain image view.");
            }
        }
    }

    /*
        Commands in Vulkan, like drawing operations and memory transfers, are not executed directly using function calls. We have to record 
        all of the operations you want to perform in command buffer objects. The advantage of this is that when we are ready to tell 
        Vulkan what we want to do, all of the commands are submitted together and Vulkan can more efficiently process the commands since 
        all of them are available together. In addition, this allows command recording to happen in multiple threads if so desired.
    */
    void createCommandPool()
    {
        QueueFamilyIndices queueFamilyIndices = findQueueFamilies(physicalDevice);

        VkCommandPoolCreateInfo poolInfo {};

        poolInfo.sType = VK_STRUCTURE_TYPE_COMMAND_POOL_CREATE_INFO;
        /*
            There are two possible flags for command pools:

            VK_COMMAND_POOL_CREATE_TRANSIENT_BIT: Hint that command buffers are rerecorded with new commands very often 
            (may change memory allocation behavior).

            VK_COMMAND_POOL_CREATE_RESET_COMMAND_BUFFER_BIT: Allow command buffers to be rerecorded individually, 
            without this flag they all have to be reset together.

            We will be recording a command buffer every frame, so we want to be able to reset and rerecord over it. 
            Thus, we need to set the VK_COMMAND_POOL_CREATE_RESET_COMMAND_BUFFER_BIT flag bit for our command pool.
        */
        poolInfo.flags = VK_COMMAND_POOL_CREATE_RESET_COMMAND_BUFFER_BIT;
        /*
            Command buffers are executed by submitting them on one of the device queues, like the graphics and presentation queues 
            we retrieved. Each command pool can only allocate command buffers that are submitted on a single type of queue. 
            We are going to record commands for drawing, which is why we have chosen the graphics queue family.
        */
        poolInfo.queueFamilyIndex = queueFamilyIndices.graphicsFamily.value();

        if (vkCreateCommandPool(device, &poolInfo, nullptr, &commandPool) != VK_SUCCESS)
        {
            throw std::runtime_error("Failed to create command pool.");
        }
    }

    void createCommandBuffers()
    {
        commandBuffers.resize(MAX_FRAMES_IN_FLIGHT);

        VkCommandBufferAllocateInfo allocInfo {};

        allocInfo.sType = VK_STRUCTURE_TYPE_COMMAND_BUFFER_ALLOCATE_INFO;
        allocInfo.commandPool = commandPool;
        /*
            The level parameter specifies if the allocated command buffers are primary or secondary command buffers.

            VK_COMMAND_BUFFER_LEVEL_PRIMARY: Can be submitted to a queue for execution, but cannot be called from other command buffers.

            VK_COMMAND_BUFFER_LEVEL_SECONDARY: Cannot be submitted directly, but can be called from primary command buffers.

            We won’t make use of the secondary command buffer functionality here, but you can 
            imagine that it’s helpful to reuse common operations from primary command buffers.
        */
        allocInfo.level = VK_COMMAND_BUFFER_LEVEL_PRIMARY;
        allocInfo.commandBufferCount = static_cast<uint32_t>(commandBuffers.size()); // allocate command buffers from the command pool

        if (vkAllocateCommandBuffers(device, &allocInfo, commandBuffers.data()) != VK_SUCCESS)
        {
            throw std::runtime_error("Failed to allocate command buffer(s).");
        }
    }

    VkCommandBuffer beginOneTimeCommands() 
    {
        VkCommandBufferAllocateInfo allocInfo {};

        allocInfo.sType = VK_STRUCTURE_TYPE_COMMAND_BUFFER_ALLOCATE_INFO;
        allocInfo.level = VK_COMMAND_BUFFER_LEVEL_PRIMARY;
        allocInfo.commandPool = commandPool;
        allocInfo.commandBufferCount = 1;

        VkCommandBuffer commandBuffer;

        vkAllocateCommandBuffers(device, &allocInfo, &commandBuffer);

        /*
            Immediately start recording the command buffer.
            We are only going to use the command buffer once and wait with returning from the function until the copy operation has
            finished executing. It’s good practice to tell the driver about our intent using VK_COMMAND_BUFFER_USAGE_ONE_TIME_SUBMIT_BIT.
        */
        VkCommandBufferBeginInfo beginInfo {};

        beginInfo.sType = VK_STRUCTURE_TYPE_COMMAND_BUFFER_BEGIN_INFO;
        beginInfo.flags = VK_COMMAND_BUFFER_USAGE_ONE_TIME_SUBMIT_BIT;

        vkBeginCommandBuffer(commandBuffer, &beginInfo);

        return commandBuffer;
    }

    void endOneTimeCommands(VkCommandBuffer commandBuffer) 
    {
        // This command buffer only contains the copy command, so we can stop recording right after.
        vkEndCommandBuffer(commandBuffer);

        VkSubmitInfo submitInfo {};

        submitInfo.sType = VK_STRUCTURE_TYPE_SUBMIT_INFO;
        submitInfo.commandBufferCount = 1;
        submitInfo.pCommandBuffers = &commandBuffer;

        vkQueueSubmit(graphicsQueue, 1, &submitInfo, VK_NULL_HANDLE);

        vkQueueWaitIdle(graphicsQueue); // enforcing one-time commands start and finish sequentially

        vkFreeCommandBuffers(device, commandPool, 1, &commandBuffer);
    }

    // writes the commands we want to execute into a command buffer
    void recordCommandBuffer(VkCommandBuffer commandBuffer, uint32_t imageIndex)
    {
        VkCommandBufferBeginInfo beginInfo {};

        beginInfo.sType = VK_STRUCTURE_TYPE_COMMAND_BUFFER_BEGIN_INFO;
        /*
            The flags parameter specifies how we’re going to use the command buffer. The following values are available:

            VK_COMMAND_BUFFER_USAGE_ONE_TIME_SUBMIT_BIT: The command buffer will be rerecorded right after executing it once.

            VK_COMMAND_BUFFER_USAGE_RENDER_PASS_CONTINUE_BIT: This is a secondary command buffer that will be entirely within a single render pass.

            VK_COMMAND_BUFFER_USAGE_SIMULTANEOUS_USE_BIT: The command buffer can be resubmitted while it is also already pending execution.

            None of these flags are applicable for us right now.
        */
        beginInfo.flags = 0; // Optional
        /*
            The pInheritanceInfo parameter is only relevant for secondary command buffers. 
            It specifies which state to inherit from the calling primary command buffers.
        */
        beginInfo.pInheritanceInfo = nullptr; // Optional

        // If the command buffer was already recorded once, then a call to vkBeginCommandBuffer will implicitly reset it. 
        // It’s not possible to append commands to a buffer at a later time.
        if (vkBeginCommandBuffer(commandBuffer, &beginInfo) != VK_SUCCESS)
        {
            throw std::runtime_error("Failed to begin recording command buffer.");
        }

        VkRenderPassBeginInfo renderPassInfo {};
        /*
            The first parameters are the render pass itself and the attachments to bind. We created a framebuffer for each swap chain image 
            where it is specified as a color attachment. Thus we need to bind the framebuffer for the swapchain image we want to draw to. 
            Using the imageIndex parameter which was passed in, we can pick the right framebuffer for the current swapchain image.
        */
        renderPassInfo.sType = VK_STRUCTURE_TYPE_RENDER_PASS_BEGIN_INFO;
        renderPassInfo.renderPass = renderPass;
        renderPassInfo.framebuffer = swapChainFramebuffers[imageIndex];
        /*
            The next parameters define the size of the render area. The render area defines where shader loads and stores will take place. 
            The pixels outside this region will have undefined values. It should match the size of the attachments for best performance.
        */
        renderPassInfo.renderArea.offset = { 0, 0 };
        renderPassInfo.renderArea.extent = swapChainExtent;

        /*
            Because we now have multiple attachments with VK_ATTACHMENT_LOAD_OP_CLEAR, we also need to specify multiple clear values.
            The range of depths in the depth buffer is 0.0 to 1.0 in Vulkan, where 1.0 lies at the far view plane and 0.0 at the 
            near view plane. The initial value at each point in the depth buffer should be the furthest possible depth, which is 1.0.
            Note that the order of clearValues should be identical to the order of attachments.
        */
        std::array<VkClearValue, 2> clearValues {};
        /*
            The last two parameters define the clear values to use for VK_ATTACHMENT_LOAD_OP_CLEAR, which we used as load
            operation for the color attachment. We set the clear color to simply be black with 100% opacity.
        */
        clearValues[0].color = {{ 0.0f, 0.0f, 0.0f, 1.0f }};
        clearValues[1].depthStencil = { 1.0f, 0 };
        
        renderPassInfo.clearValueCount = static_cast<uint32_t>(clearValues.size());
        renderPassInfo.pClearValues = clearValues.data();

        /*
            All of the functions that record commands can be recognized by their vkCmd prefix. 
            They all return void, so there will be no error handling until we’ve finished recording.
            The first parameter for every command is always the command buffer to record the command to. 
            The second parameter specifies the details of the render pass.. 
            The final parameter controls how the drawing commands within the render pass will be provided. It can have one of two values:

            VK_SUBPASS_CONTENTS_INLINE: The render pass commands will be embedded in the primary command buffer itself and no secondary command buffers will be executed.

            VK_SUBPASS_CONTENTS_SECONDARY_COMMAND_BUFFERS: The render pass commands will be executed from secondary command buffers.

            We will not be using secondary command buffers, so we will go with the first option.
        */
        vkCmdBeginRenderPass(commandBuffer, &renderPassInfo, VK_SUBPASS_CONTENTS_INLINE);

        /*
            Binds the graphics pipeline. The second parameter specifies if the pipeline object is a graphics or compute pipeline. 
            We’ve now told Vulkan which operations to execute in the graphics pipeline and which attachment to use in the fragment shader.
        */
        vkCmdBindPipeline(commandBuffer, VK_PIPELINE_BIND_POINT_GRAPHICS, graphicsPipeline);

        /*
            In the createGraphicsPipeline method we specified viewport and scissor state to be dynamic. 
            Therefore, we need to set them in the command buffer before issuing our draw command.
        */
        VkViewport viewport {};

        viewport.x = 0.0f;
        viewport.y = 0.0f;
        viewport.width = static_cast<float>(swapChainExtent.width);
        viewport.height = static_cast<float>(swapChainExtent.height);
        viewport.minDepth = 0.0f;
        viewport.maxDepth = 1.0f;

        vkCmdSetViewport(commandBuffer, 0, 1, &viewport);
        
        /*
            While viewports define the transformation from the image to the framebuffer, scissor rectangles define in
            which regions pixels will actually be stored. Any pixels outside the scissor rectangles will be discarded
            by the rasterizer. They function like a filter rather than a transformation.
            If we want to draw to the entire framebuffer, specify a scissor rectangle that covers it entirely.
        */
        VkRect2D scissor {};

        scissor.offset = { 0, 0 };
        scissor.extent = swapChainExtent;
        
        vkCmdSetScissor(commandBuffer, 0, 1, &scissor);

        // A graphics pipeline supports binding multiple buffers simultaneously (must match VkPipelineVertexInputStateCreateInfo)
        VkBuffer vertexBuffers[] = { vertexBuffer };
        VkDeviceSize offsets[] = { 0 };

        // binds buffers to a command buffer for use in subsequent drawing commands
        vkCmdBindVertexBuffers(commandBuffer, 0, 1, vertexBuffers, offsets);
        vkCmdBindIndexBuffer(commandBuffer, indexBuffer, 0, VK_INDEX_TYPE_UINT32); // we can only have a single index buffer

        /*
            Unlike vertex and index buffers, descriptor sets are not unique to graphics pipelines. Therefore, we need to specify if 
            we want to bind descriptor sets to the graphics or compute pipeline. The next parameter is the layout that the descriptors 
            are based on. The next three parameters specify the index of the first descriptor set, the number of sets to bind, and the 
            array of sets to bind (there could be multiple descriptor sets per frame). The last two parameters specify an array of offsets 
            that are used for dynamic descriptors.
        */
        vkCmdBindDescriptorSets(commandBuffer, VK_PIPELINE_BIND_POINT_GRAPHICS, pipelineLayout, 0, 1, &descriptorSets[currentFrame], 0, nullptr);

        /*
            The first two parameters specify the number of indices and the number of instances. We’re not using instancing, so just 
            specify 1 instance. The number of indices represents the number of vertices that will be passed to the vertex shader. 
            The next parameter specifies an offset into the index buffer, using a value of 1 would cause the graphics card to 
            start reading at the second index. The second to last parameter specifies an offset to add to the indices in the index buffer. 
            The final parameter specifies an offset for instancing, which we’re not using.
        */
        vkCmdDrawIndexed(commandBuffer, static_cast<uint32_t>(indices.size()), 1, 0, 0, 0);

        vkCmdEndRenderPass(commandBuffer);

        if (vkEndCommandBuffer(commandBuffer) != VK_SUCCESS)
        {
            throw std::runtime_error("Failed to record command buffer.");
        }
    }

    /*
        We need one semaphore to signal that an image has been acquired from the swapchain and is ready for rendering, 
        another one to signal that rendering has finished and presentation can happen, 
        and a fence to make sure only one frame is rendering at a time.
    */
    void createSyncObjects()
    {
        imageAvailableSemaphores.resize(MAX_FRAMES_IN_FLIGHT);
        renderFinishedSemaphores.resize(MAX_FRAMES_IN_FLIGHT);
        inFlightFences.resize(MAX_FRAMES_IN_FLIGHT);

        // Future versions of the Vulkan API or extensions may add functionality for the flags and pNext parameters.
        VkSemaphoreCreateInfo semaphoreInfo {};
        semaphoreInfo.sType = VK_STRUCTURE_TYPE_SEMAPHORE_CREATE_INFO;

        VkFenceCreateInfo fenceInfo {};
        fenceInfo.sType = VK_STRUCTURE_TYPE_FENCE_CREATE_INFO;
        /*
            On the first frame we call drawFrame(), which immediately waits on inFlightFence to be signaled. 
            inFlightFence is only signaled after a frame has finished rendering, yet since this is the first frame, 
            there are no previous frames in which to signal the fence. Therefore, create the fence in the signaled state
            so that the first call to vkWaitForFences() returns immediately since the fence is already signaled.
        */
        fenceInfo.flags = VK_FENCE_CREATE_SIGNALED_BIT; // unblocked initially to allow first draw

        for (size_t i = 0; i < MAX_FRAMES_IN_FLIGHT; i++) 
        {
            if (vkCreateSemaphore(device, &semaphoreInfo, nullptr, &imageAvailableSemaphores[i]) != VK_SUCCESS ||
                vkCreateSemaphore(device, &semaphoreInfo, nullptr, &renderFinishedSemaphores[i]) != VK_SUCCESS ||
                vkCreateFence(device, &fenceInfo, nullptr, &inFlightFences[i]) != VK_SUCCESS)
            {
                throw std::runtime_error("Failed to create synchronization objects for a frame.");
            }
        }
    }

    /*
        Generates a new transformation every frame (i.e. to make the geometry spin).
        Using a UBO this way is not the most efficient way to pass frequently changing values to the shader. 
        A more efficient way to pass a small buffer of data to shaders are push constants.
    */
    void updateUniformBuffer(uint32_t currentImage)
    {
        /*
            The static keyword is used to initialize the variable exactly once and maintain its value between function calls
            The static keyword effectively turns startTime into a global variable in the scope of the updateUniformBuffer function. 
            This technique is often used to track the state that persists across function calls 
            without using global variables accessible outside the function scope.
        */
        static auto startTime = std::chrono::high_resolution_clock::now();

        auto currentTime = std::chrono::high_resolution_clock::now();
        // calculate the time in seconds since rendering has started with floating point accuracy
        float deltaTime = std::chrono::duration<float, std::chrono::seconds::period>(currentTime - startTime).count();

        UniformBufferObject ubo {};
        /*
            Takes an existing transformation, rotation angle, and rotation axis as parameters. 
            The glm::mat4(1.0f) constructor returns an identity matrix. Using a rotation angle of deltaTime * glm::radians(90.0f) 
            accomplishes the purpose of rotation of 90 degrees per second, regardless of frame rate.
            If deltaTime = 1 second, then 90 degrees rotation. If deltaTime = 0.5 second, then 45 degrees rotation, etc.
        */
        ubo.model = glm::rotate(glm::mat4(1.0f), deltaTime * glm::radians(90.0f), glm::vec3(0.0f, 0.0f, 1.0f));
        //ubo.model = glm::mat4(1.0f);
        // Looks at the geometry from above at a 45 degree angle. It takes the eye position, center position, and up axis as parameters.
        ubo.view = glm::lookAt(glm::vec3(2.0f, 2.0f, 2.0f), glm::vec3(0.0f, 0.0f, 0.0f), glm::vec3(0.0f, 0.0f, 1.0f));
        /*
            Perspective projection with a 45 degree vertical field-of-view with the aspect ratio of the viewport.
            The glm::perspective matrix defines how the scene is projected onto a 2D viewport from a specific viewing angle.
            FOV influences how wide the scene appears and how pronounced is the perspective effect (objects appear smaller as they are further away).
            The aspect ratio parameter ensures that the scene is correctly scaled horizontally and vertically, preventing distortion.
            For example, a widescreen display would have a greater width than height, leading to a larger aspect ratio, ensuring that
            objects in the scene have the correct proportions regardless of the screen size or shape.
            The other parameters are the near and far view planes. These values define the depth range of the scene that will be rendered. 
            Objects closer than the near plane or further than the far plane are clipped out and not displayed. It is important to use the 
            current swap chain extent to calculate the aspect ratio to take into account the new width and height of the window after a resize.
        */
        ubo.projection = glm::perspective(glm::radians(45.0f), swapChainExtent.width / (float)swapChainExtent.height, 0.1f, 10.0f);
        /*
            GLM was originally designed for OpenGL, where the Y coordinate of the clip coordinates is inverted. 
            The easiest way to compensate for that is to flip the sign on the scaling factor of the Y axis in the projection matrix. 
            If we don’t do this, then the image will be rendered upside down.
        */
        ubo.projection[1][1] *= -1;
        /*
            Simplified form of a perspective projection matrix:

            | a  0  0  0 |
            | 0  b  0  0 |
            | 0  0  c  d |
            | 0  0 -1  0 |

            a: This is typically found in the first row, first column (matrix[0][0]). It's related to the aspect ratio of the viewport and the field of view (FOV). It determines how much the scene is scaled horizontally. For a perspective projection, a is calculated as 1 / tan(FOVx / 2), where FOVx is the horizontal field of view. If the matrix is configured using the vertical FOV (which is common), a would also incorporate the aspect ratio of the viewport.

            b: This entry (matrix[1][1]) is directly related to the vertical scaling of the scene and is determined by the vertical field of view (FOVy). It is calculated as 1 / tan(FOVy / 2). A larger FOVy results in a smaller value for b, causing less vertical scaling, and thus objects appear larger. It directly affects how "zoomed in" the scene appears vertically.

            c and d: These are found in the third row and column (matrix[2][2]) and the third row, fourth column (matrix[2][3]), respectively. They are related to the near and far clipping planes (n and f). These values determine the mapping of depth to the normalized device coordinates and are crucial for depth buffering. They help ensure that only objects within a certain range of distances from the camera are rendered. The exact formulas for c and d depend on whether the depth range is 0 to 1 (Direct3D style) or -1 to 1 (traditional OpenGL style).

            -1 in matrix[3][2]: This entry is part of what performs the "perspective divide" in the projection transformation. It's what creates the perspective effect, causing objects to appear smaller as they get farther from the camera.
        */

        // copy the data in the uniform buffer object to the current (mapped) uniform buffer
        memcpy(uniformBuffersMapped[currentImage], &ubo, sizeof(ubo));

        // ALWAYS REMEMBER TO RECOMPILE THE SHADERS IF THEY CHANGED!
    }

    void cleanupSwapChain()
    {
        for (auto framebuffer : swapChainFramebuffers) 
        {
            vkDestroyFramebuffer(device, framebuffer, nullptr);
        }

        for (auto imageView : swapChainImageViews) 
        {
            vkDestroyImageView(device, imageView, nullptr);
        }

        vkDestroyImageView(device, depthImageView, nullptr);
        vkDestroyImage(device, depthImage, nullptr);
        vkFreeMemory(device, depthImageMemory, nullptr);

        vkDestroySwapchainKHR(device, swapChain, nullptr);
    }

    void recreateSwapChain()
    {
        int width = 0, height = 0;
        glfwGetFramebufferSize(window, &width, &height);
        /*
            There is another case where a swap chain may become out of date and that is a special kind of window resizing: 
            window minimization. This case is special because it will result in a frame buffer size of 0.
            We handle this by pausing until the window is in the foreground again.
        */
        while (width == 0 && height == 0)
        {
            glfwWaitEvents();
            glfwGetFramebufferSize(window, &width, &height);
        }

        vkDeviceWaitIdle(device);

        cleanupSwapChain();

        createSwapChain();
        createImageViews(); // need to be recreated because they are based directly on the swap chain images
        createDepthResources(); // needs to be recreated due to specific swap chain extent requirements
        /*
            Note that we don’t recreate the render pass here for simplicity. In theory it can be possible for the swap chain image format 
            to change during an applications' lifetime, e.g. when moving a window from an standard range to an high dynamic range monitor. 
            This may require the application to recreate the renderpass to make sure the change between dynamic ranges is properly reflected.
        */
        createFramebuffers(); // need to be recreated because they directly depend on the swap chain images (and depth image)
    }

    void initVulkan() 
    {
        createInstance();
        setupDebugMessenger();
        createSurface();
        selectPhysicalDevice();
        createLogicalDevice();
        createSwapChain();
        createImageViews();
        createRenderPass();
        createDescriptorSetLayout();
        createGraphicsPipeline(); // requires descriptor set layout
        createCommandPool();
        createDepthResources();
        createFramebuffers(); // depends on depth image and swap chain images
        createTextureImage(); // requires command buffers
        createTextureImageView();
        createTextureSampler();
        loadModel();
        createVertexBuffer(); // requires the command pool due to the memory transfer command
        createIndexBuffer();
        createUniformBuffers();
        createDescriptorPool();
        createDescriptorSets(); // requires descriptor set layout(s)
        createCommandBuffers();
        createSyncObjects();
    }

    /*
        Rendering a frame in Vulkan consists of a common set of steps:

        1. Wait for the previous frame to finish
        2. Acquire an image from the swap chain
        3. Record a command buffer which draws the scene onto that image
        4. Submit (execute) the recorded command buffer
        5. Return the finished image to the swap chain and present it on the surface

        All GPU commands are executed asynchronously.
    */
    void drawFrame()
    {
        /*
            Takes an array of fences and waits on the host for either any or all of the fences to be signaled before returning. 
            The VK_TRUE indicates that we want to wait for all fences, but in the case of a single one it doesn’t matter. We set the 
            timeout parameter to the maximum value of a 64 bit unsigned integer, UINT64_MAX, which effectively disables the timeout.
        */
        vkWaitForFences(device, 1, &inFlightFences[currentFrame], VK_TRUE, UINT64_MAX);

        uint32_t imageIndex;
        /* 
            Since the swap chain is an extension feature, we must use a function with the vk* KHR naming convention.
            Parameters 4 and 5 are synchronization objects that are to be signaled (unblocked) when the presentation 
            engine is finished using the image. That’s the point in time where we can start drawing to it.
            The index refers to the VkImage in our swapChainImages array. We are going to use that index to pick the VkFrameBuffer.
            UINT64_MAX means the function will block indefinitely until an image becomes available.
        */
        VkResult result = vkAcquireNextImageKHR(device, swapChain, UINT64_MAX, imageAvailableSemaphores[currentFrame], VK_NULL_HANDLE, &imageIndex);

        /*
            The vkAcquireNextImageKHR and vkQueuePresentKHR functions can return the following special values to indicate this.

            VK_ERROR_OUT_OF_DATE_KHR: The swap chain has become incompatible with the surface 
            and can no longer be used for rendering. Usually happens after a window resize.

            VK_SUBOPTIMAL_KHR: The swap chain can still be used to successfully present 
            to the surface, but the surface properties are no longer matched exactly.
        */
        if (result == VK_ERROR_OUT_OF_DATE_KHR)
        {
            recreateSwapChain();
            return;
        }
        else if (result != VK_SUCCESS && result != VK_SUBOPTIMAL_KHR)
        {
            throw std::runtime_error("Failed to acquire swap chain image.");
        }

        // As long as we are done using the data (based on imageAvailableSemaphores[currentFrame]), we can overwrite it.
        updateUniformBuffer(currentFrame); // per frame UBO changes every frame

        // Reset the fence to the unsignaled (blocked) state only if we are submitting work (avoids deadlock).
        vkResetFences(device, 1, &inFlightFences[currentFrame]);

        /*
            With the imageIndex specifying the swap chain image to use in hand, we can now record the command buffer. 
            First, we call vkResetCommandBuffer on the command buffer to make sure it is able to be recorded.
            The second parameter of vkResetCommandBuffer is a VkCommandBufferResetFlagBits flag. 
            Since we don’t want to do anything special, we leave it as 0.
        */
        vkResetCommandBuffer(commandBuffers[currentFrame], 0);

        recordCommandBuffer(commandBuffers[currentFrame], imageIndex);

        // Configures queue submission and synchronization
        VkSubmitInfo submitInfo {};
        submitInfo.sType = VK_STRUCTURE_TYPE_SUBMIT_INFO;

        VkSemaphore waitSemaphores[] = { imageAvailableSemaphores[currentFrame] };
        VkPipelineStageFlags waitStages[] = { VK_PIPELINE_STAGE_COLOR_ATTACHMENT_OUTPUT_BIT };
        /*
            The first three parameters specify which semaphores to wait on before execution begins and in which stage(s) of the 
            pipeline to wait. We want to wait with writing colors to the image until it’s available, so we’re specifying the 
            stage of the graphics pipeline that writes to the color attachment. That means that theoretically the implementation 
            can already start executing our vertex shader and such while the image is not yet available. Each entry in the 
            waitStages array corresponds to the semaphore with the same index in pWaitSemaphores.
        */
        submitInfo.waitSemaphoreCount = 1;
        submitInfo.pWaitSemaphores = waitSemaphores;
        submitInfo.pWaitDstStageMask = waitStages;

        // These specify which command buffers to actually submit for execution. We simply submit the single command buffer we have.
        submitInfo.commandBufferCount = 1;
        submitInfo.pCommandBuffers = &commandBuffers[currentFrame];

        VkSemaphore signalSemaphores[] = { renderFinishedSemaphores[currentFrame] };
        // These specify which semaphores to signal once the command buffer(s) have finished execution.
        submitInfo.signalSemaphoreCount = 1;
        submitInfo.pSignalSemaphores = signalSemaphores;

        /*
            Takes an array of VkSubmitInfo structures as argument for efficiency when the workload is much larger. 
            The last parameter references an optional fence that will be signaled when the command buffers finish execution. 
            This allows us to know when it is safe for the command buffer to be reused, thus we want to give it inFlightFence. 
            Now on the next frame, the CPU will wait for this command buffer to finish executing before it records new commands into it.
        */
        if (vkQueueSubmit(graphicsQueue, 1, &submitInfo, inFlightFences[currentFrame]) != VK_SUCCESS)
        {
            throw std::runtime_error("Failed to submit draw command buffer.");
        }

        VkPresentInfoKHR presentInfo {};

        presentInfo.sType = VK_STRUCTURE_TYPE_PRESENT_INFO_KHR;
        /*
            The first two parameters specify which semaphores to wait on before presentation can happen, just like VkSubmitInfo. 
            Since we want to wait on the command buffer to finish execution, thus our triangle being drawn, 
            we take the semaphores which will be signalled and wait on them, thus we use signalSemaphores.
        */
        presentInfo.waitSemaphoreCount = 1;
        presentInfo.pWaitSemaphores = signalSemaphores; // present until rendering is complete (commands done)

        VkSwapchainKHR swapChains[] = { swapChain };
        // Specify the swap chains to present images to and the index of the image for each swap chain. This will almost always be one.
        presentInfo.swapchainCount = 1;
        presentInfo.pSwapchains = swapChains;
        presentInfo.pImageIndices = &imageIndex;
        /*
            Allows us to specify an array of VkResult values to check for every individual swap chain if presentation was successful. 
            It’s not necessary if we are only using a single swap chain because we can simply use the return value of the present function.
        */
        presentInfo.pResults = nullptr; // Optional

        // Submits the request to present an image to the swap chain
        result = vkQueuePresentKHR(presentQueue, &presentInfo);

        // In this case we will also recreate the swap chain if it is suboptimal because we want the best possible result.
        if (result == VK_ERROR_OUT_OF_DATE_KHR || result == VK_SUBOPTIMAL_KHR || framebufferResized) 
        {
            /*
                It is important to do this after vkQueuePresentKHR to ensure that the semaphores are in 
                a consistent state, otherwise a signaled semaphore may never be properly waited upon.
            */
            framebufferResized = false;
            recreateSwapChain();
        }
        else if (result != VK_SUCCESS) 
        {
            throw std::runtime_error("Failed to present swap chain image.");
        }

        // By using the modulo (%) operator, we ensure that the frame index loops around after every MAX_FRAMES_IN_FLIGHT enqueued frames.
        currentFrame = (currentFrame + 1) % MAX_FRAMES_IN_FLIGHT;
    }

    void mainLoop() 
    {
        while (!glfwWindowShouldClose(window))
        {
            glfwPollEvents();

            drawFrame();
        }

        /*
            All of the operations in drawFrame are asynchronous. That means that when we exit the loop in mainLoop, 
            drawing and presentation operations may still be going on. Cleaning up resources while that is happening is a bad idea.
            To fix that problem, wait for the logical device to finish operations before exiting mainLoop and destroying the window.
            You can also wait for operations in a specific command queue to be finished with vkQueueWaitIdle.
        */
        vkDeviceWaitIdle(device);
    }

    // Note the reverse order of deletions based on dependencies
    void cleanup() 
    {
        cleanupSwapChain();

        vkDestroySampler(device, textureSampler, nullptr);
        vkDestroyImageView(device, textureImageView, nullptr); // destroy before the image
        vkDestroyImage(device, textureImage, nullptr);
        vkFreeMemory(device, textureImageMemory, nullptr);
        // Memory that is bound to a buffer object may be freed once the buffer is no longer used.
        vkDestroyBuffer(device, indexBuffer, nullptr);
        vkFreeMemory(device, indexBufferMemory, nullptr);
        vkDestroyBuffer(device, vertexBuffer, nullptr);
        vkFreeMemory(device, vertexBufferMemory, nullptr);

        for (size_t i = 0; i < MAX_FRAMES_IN_FLIGHT; i++) 
        {
            vkDestroyBuffer(device, uniformBuffers[i], nullptr);
            vkFreeMemory(device, uniformBuffersMemory[i], nullptr);
        }

        vkDestroyDescriptorPool(device, descriptorPool, nullptr); // automatically frees descriptor sets
        vkDestroyDescriptorSetLayout(device, descriptorSetLayout, nullptr);
        vkDestroyPipeline(device, graphicsPipeline, nullptr);
        vkDestroyPipelineLayout(device, pipelineLayout, nullptr);
        vkDestroyRenderPass(device, renderPass, nullptr);

        for (size_t i = 0; i < MAX_FRAMES_IN_FLIGHT; i++) 
        {
            vkDestroySemaphore(device, imageAvailableSemaphores[i], nullptr);
            vkDestroySemaphore(device, renderFinishedSemaphores[i], nullptr);
            vkDestroyFence(device, inFlightFences[i], nullptr);
        }

        vkDestroyCommandPool(device, commandPool, nullptr);
        // Logical devices don’t interact directly with instances, which is why instance is not included as a parameter.
        vkDestroyDevice(device, nullptr);

        if (enableValidationLayers) 
        {
            DestroyDebugUtilsMessengerEXT(instance, debugMessenger, nullptr);
        }

        vkDestroySurfaceKHR(instance, surface, nullptr);
        vkDestroyInstance(instance, nullptr);

        glfwDestroyWindow(window);
        glfwTerminate();
    }
};

int main() 
{
    HelloTriangleApplication app;

    try 
    {
        app.run();
    }
    catch (const std::exception& e) 
    {
        std::cerr << e.what() << std::endl;
        return EXIT_FAILURE;
    }

    return EXIT_SUCCESS;
}

/*
    Object/Model Space: Coordinates are defined relative to the local origin of the geometric object. This is where vertices of a model typically start.

    World Space: After applying the model transformation, coordinates are in a common space shared by all objects in the scene.

    View/Camera Space: Applying the view transformation positions objects relative to the camera's viewpoint.

    Clip Space: After the projection transformation, coordinates are in a space where they can be clipped against the view frustum. Vertices are then perspective-divided to go from clip space to NDC.

    NDC (Normalized Device Coordinates): In this space, the x, y, and z coordinates are normalized to be within a standard cubic volume (in Vulkan, x and y range from -1.0 to 1.0, and z from 0.0 to 1.0 for the default depth range). This space is what the viewport transformation uses to map to framebuffer coordinates.

    Framebuffer/Window Space: Finally, the viewport transformation maps NDC to the actual pixels in the framebuffer or the window surface being rendered to.

    NDC is a crucial step in the pipeline that standardizes how geometry is represented before it’s mapped onto the screen or the target framebuffer, ensuring consistent rendering across different hardware and graphics APIs.
*/